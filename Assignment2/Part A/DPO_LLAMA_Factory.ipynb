{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a0ec23e7cd74886a8f1709de54a27b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54c538728ce44c308164b40edde6e490",
              "IPY_MODEL_4d52bdcef49647db9f7b4469a72cbcc1",
              "IPY_MODEL_4d7a07ab4fc14eb3b05faa044402fa66"
            ],
            "layout": "IPY_MODEL_a23e58b6eef0452d94a880bbd918d115"
          }
        },
        "54c538728ce44c308164b40edde6e490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd74715d6d94d4da029dda8421a3c3e",
            "placeholder": "​",
            "style": "IPY_MODEL_e8826fd89651407b9817af2808eaf357",
            "value": "Downloading readme: 100%"
          }
        },
        "4d52bdcef49647db9f7b4469a72cbcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05989ddcaa04617973910b1aec37dd4",
            "max": 8625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da7e6656aa5540cf8c7bdb72a2175c51",
            "value": 8625
          }
        },
        "4d7a07ab4fc14eb3b05faa044402fa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2905c730f74a4ab1af6a3ea973cba13c",
            "placeholder": "​",
            "style": "IPY_MODEL_86bf4837eb1d4ae6a4694092e60a8050",
            "value": " 8.62k/8.62k [00:00&lt;00:00, 30.7kB/s]"
          }
        },
        "a23e58b6eef0452d94a880bbd918d115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd74715d6d94d4da029dda8421a3c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8826fd89651407b9817af2808eaf357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05989ddcaa04617973910b1aec37dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7e6656aa5540cf8c7bdb72a2175c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2905c730f74a4ab1af6a3ea973cba13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86bf4837eb1d4ae6a4694092e60a8050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07dacf531c294914a1a885e5242c4e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38f75db77c3b4b41bc6e547ab2464faf",
              "IPY_MODEL_fa5131d7a38d469383dd036bc53ad195",
              "IPY_MODEL_93c0e1db668b463cab792b3067335c2a"
            ],
            "layout": "IPY_MODEL_cd946530c65c47efaab6bbc06961e86c"
          }
        },
        "38f75db77c3b4b41bc6e547ab2464faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb8aa9368a443df83bdc60183fc5383",
            "placeholder": "​",
            "style": "IPY_MODEL_eef6c709b8ff44c6acaea01d81886be0",
            "value": "Downloading data: 100%"
          }
        },
        "fa5131d7a38d469383dd036bc53ad195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ade08f73c7e441dbbc60ab647fb79342",
            "max": 109861341,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67bdc9252ca642f09e3a7894709fece5",
            "value": 109861341
          }
        },
        "93c0e1db668b463cab792b3067335c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc023829157f46da97eef5e5c3df8231",
            "placeholder": "​",
            "style": "IPY_MODEL_0c2d681186b94fc0bd7a70b4472c4897",
            "value": " 110M/110M [00:05&lt;00:00, 24.0MB/s]"
          }
        },
        "cd946530c65c47efaab6bbc06961e86c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb8aa9368a443df83bdc60183fc5383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef6c709b8ff44c6acaea01d81886be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ade08f73c7e441dbbc60ab647fb79342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67bdc9252ca642f09e3a7894709fece5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc023829157f46da97eef5e5c3df8231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c2d681186b94fc0bd7a70b4472c4897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d0b4faa4db348a7a8d9f53f22801d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c54e149986045e5918e9524b87d500e",
              "IPY_MODEL_fd4110215bb14eb6a50c34536dee571c",
              "IPY_MODEL_9c73e694411545aebb18b3a3646ed1e2"
            ],
            "layout": "IPY_MODEL_13cf54651ec44fb8a81c9d6a2da62797"
          }
        },
        "8c54e149986045e5918e9524b87d500e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff66deb1648b499892421a470405ee03",
            "placeholder": "​",
            "style": "IPY_MODEL_6fe2e1f5cef0405481285cd080335b65",
            "value": "Generating train split: 100%"
          }
        },
        "fd4110215bb14eb6a50c34536dee571c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93ddcc46c35449db385b688a24acb6b",
            "max": 63619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_692955efd8ea4d288a48e661143996fc",
            "value": 63619
          }
        },
        "9c73e694411545aebb18b3a3646ed1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4753e03748f40649d30bcf8a2147460",
            "placeholder": "​",
            "style": "IPY_MODEL_cee7e10c29774bb19f84d0658d4eea6f",
            "value": " 63619/63619 [00:01&lt;00:00, 67724.21 examples/s]"
          }
        },
        "13cf54651ec44fb8a81c9d6a2da62797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff66deb1648b499892421a470405ee03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fe2e1f5cef0405481285cd080335b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e93ddcc46c35449db385b688a24acb6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692955efd8ea4d288a48e661143996fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4753e03748f40649d30bcf8a2147460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee7e10c29774bb19f84d0658d4eea6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "156a7e64dea645159e5d0c5901741cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f9f208c5a274ba49c01d3c9e627b745",
              "IPY_MODEL_945b4cfb31c64367ae15b257f60bfa97",
              "IPY_MODEL_0ef6909f6cc146b6960b8dbaa6e506b0"
            ],
            "layout": "IPY_MODEL_7b9559d61d4c46c7a67b8aa22de1ec43"
          }
        },
        "7f9f208c5a274ba49c01d3c9e627b745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f42338b96004bd0a0b3934fb2400fc2",
            "placeholder": "​",
            "style": "IPY_MODEL_b2cfaece17c0468f8bacfbef166e1ce8",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "945b4cfb31c64367ae15b257f60bfa97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99bfeb0472c94f44b2b9178fe39fb241",
            "max": 63619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43ada6b858fd4717ba23285934f5f8be",
            "value": 63619
          }
        },
        "0ef6909f6cc146b6960b8dbaa6e506b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b45a6b8749d481f9b55a4cc5c492cab",
            "placeholder": "​",
            "style": "IPY_MODEL_b50fb90fa4e248a4a5169d1951b28ec2",
            "value": " 63619/63619 [00:00&lt;00:00, 132174.88 examples/s]"
          }
        },
        "7b9559d61d4c46c7a67b8aa22de1ec43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f42338b96004bd0a0b3934fb2400fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2cfaece17c0468f8bacfbef166e1ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99bfeb0472c94f44b2b9178fe39fb241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ada6b858fd4717ba23285934f5f8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b45a6b8749d481f9b55a4cc5c492cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50fb90fa4e248a4a5169d1951b28ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Q783Fazk9-",
        "outputId": "49dc0453-f25b-4d70-c7ab-d548ed0e9417"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 315, done.\u001b[K\n",
            "remote: Counting objects: 100% (315/315), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 315 (delta 78), reused 195 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (315/315), 9.00 MiB | 4.94 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Collecting torch==2.3.1\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.1\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.1\n",
            "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (10.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.0.50\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.0.50:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.0.50\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1+cu121\n",
            "    Uninstalling torchvision-0.19.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.1+cu121\n",
            "    Uninstalling torchaudio-2.4.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.4.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1 triton-2.3.1\n",
            "Found existing installation: jax 0.4.33\n",
            "Uninstalling jax-0.4.33:\n",
            "  Successfully uninstalled jax-0.4.33\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.45.2,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.2)\n",
            "Collecting datasets<=2.21.0,>=2.16.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio<5.0.0,>=4.0.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.1.dev0)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.9.1.dev0)\n",
            "  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.9.2)\n",
            "Collecting fastapi (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.1.dev0)\n",
            "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n",
            "Collecting av (from llamafactory==0.9.1.dev0)\n",
            "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Collecting liger-kernel (from llamafactory==0.9.1.dev0)\n",
            "  Downloading liger_kernel-0.3.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
            "Collecting xxhash (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.10)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting orjson~=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (10.4.0)\n",
            "Collecting pydub (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.2.3)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi->llamafactory==0.9.1.dev0)\n",
            "  Downloading starlette-0.39.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.1.dev0) (12.6.77)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.2,>=4.41.2->llamafactory==0.9.1.dev0) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.2,>=4.41.2->llamafactory==0.9.1.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n",
            "  Downloading tyro-0.8.12-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.1.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (13.9.2)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.18.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading liger_kernel-0.3.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.39.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.12-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22656 sha256=e59e0f7e319830cd5f098b5d5f5e686e5400f3e5dfdda3adcd6b1b8dff04e785\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dp4jldnv/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=b0dbae39a6f67393a85f27c16455b407e0dc7ca4f28610859804350d3cac9ae6\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, websockets, tomlkit, shtab, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, fire, ffmpy, dill, av, aiofiles, uvicorn, tiktoken, starlette, multiprocess, httpcore, tyro, sse-starlette, httpx, fastapi, liger-kernel, gradio-client, bitsandbytes, peft, gradio, datasets, trl, llamafactory\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.1\n",
            "    Uninstalling MarkupSafe-3.0.1:\n",
            "      Successfully uninstalled MarkupSafe-3.0.1\n",
            "Successfully installed aiofiles-23.2.1 av-13.1.0 bitsandbytes-0.44.1 datasets-2.21.0 dill-0.3.8 fastapi-0.115.2 ffmpy-0.4.0 fire-0.7.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 liger-kernel-0.3.1 llamafactory-0.9.1.dev0 markupsafe-2.1.5 multiprocess-0.70.16 orjson-3.10.7 peft-0.12.0 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 starlette-0.39.2 tiktoken-0.8.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.12 uvicorn-0.31.1 websockets-12.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "6BRkgNAIy9GM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QknG3ucu0Kpo",
        "outputId": "13bd2e59-eca3-4190-fd2d-6abe2aff5d7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsxCuW4S2BAh",
        "outputId": "42a36ef2-2205-42de-e4d2-17adc1a73a43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bb5oinVVaxo",
        "outputId": "9cdec068-aa73-4642-cc53-f9e84cb96ff7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"argilla/ultrafeedback-binarized-preferences\", split=\"train\")\n",
        "\n",
        "# Save the dataset locally\n",
        "dataset.save_to_disk(\"ultrafeedback_dataset\")\n",
        "\n",
        "# Print the first example and column names to verify the structure\n",
        "print(dataset[0])\n",
        "print(dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "3a0ec23e7cd74886a8f1709de54a27b1",
            "54c538728ce44c308164b40edde6e490",
            "4d52bdcef49647db9f7b4469a72cbcc1",
            "4d7a07ab4fc14eb3b05faa044402fa66",
            "a23e58b6eef0452d94a880bbd918d115",
            "4fd74715d6d94d4da029dda8421a3c3e",
            "e8826fd89651407b9817af2808eaf357",
            "d05989ddcaa04617973910b1aec37dd4",
            "da7e6656aa5540cf8c7bdb72a2175c51",
            "2905c730f74a4ab1af6a3ea973cba13c",
            "86bf4837eb1d4ae6a4694092e60a8050",
            "07dacf531c294914a1a885e5242c4e5c",
            "38f75db77c3b4b41bc6e547ab2464faf",
            "fa5131d7a38d469383dd036bc53ad195",
            "93c0e1db668b463cab792b3067335c2a",
            "cd946530c65c47efaab6bbc06961e86c",
            "bdb8aa9368a443df83bdc60183fc5383",
            "eef6c709b8ff44c6acaea01d81886be0",
            "ade08f73c7e441dbbc60ab647fb79342",
            "67bdc9252ca642f09e3a7894709fece5",
            "bc023829157f46da97eef5e5c3df8231",
            "0c2d681186b94fc0bd7a70b4472c4897",
            "1d0b4faa4db348a7a8d9f53f22801d31",
            "8c54e149986045e5918e9524b87d500e",
            "fd4110215bb14eb6a50c34536dee571c",
            "9c73e694411545aebb18b3a3646ed1e2",
            "13cf54651ec44fb8a81c9d6a2da62797",
            "ff66deb1648b499892421a470405ee03",
            "6fe2e1f5cef0405481285cd080335b65",
            "e93ddcc46c35449db385b688a24acb6b",
            "692955efd8ea4d288a48e661143996fc",
            "b4753e03748f40649d30bcf8a2147460",
            "cee7e10c29774bb19f84d0658d4eea6f",
            "156a7e64dea645159e5d0c5901741cfb",
            "7f9f208c5a274ba49c01d3c9e627b745",
            "945b4cfb31c64367ae15b257f60bfa97",
            "0ef6909f6cc146b6960b8dbaa6e506b0",
            "7b9559d61d4c46c7a67b8aa22de1ec43",
            "9f42338b96004bd0a0b3934fb2400fc2",
            "b2cfaece17c0468f8bacfbef166e1ce8",
            "99bfeb0472c94f44b2b9178fe39fb241",
            "43ada6b858fd4717ba23285934f5f8be",
            "1b45a6b8749d481f9b55a4cc5c492cab",
            "b50fb90fa4e248a4a5169d1951b28ec2"
          ]
        },
        "outputId": "f6ba4a30-fa99-41d5-a29a-85a855801b61",
        "id": "eoitB0DnKsCj"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a0ec23e7cd74886a8f1709de54a27b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/110M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07dacf531c294914a1a885e5242c4e5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/63619 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d0b4faa4db348a7a8d9f53f22801d31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/63619 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156a7e64dea645159e5d0c5901741cfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'evol_instruct', 'instruction': 'Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here\\'s some starter code to help you out:\\n#include <iostream>\\n#include <string>\\nusing namespace std;\\nint main() {\\n    string country;\\n    // prompt user for input\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n    // check if country borders the Mediterranean Sea\\n    // [C++ code]\\n    return 0;\\n}', 'chosen_response': 'Here\\'s a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\\n\\n#include <iostream>\\n#include <string>\\n#include <set>\\n#include <map>\\n#include <algorithm>\\n\\nusing namespace std;\\n\\nint main() {\\n    // store countries and their bordering seas in a map\\n    map<string, set<string>> countries;\\n    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\\n    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\\n    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\\n    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n\\n    string country;\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n\\n    // check if country is in the map\\n    if (countries.find(country)!= countries.end()) {\\n        // check if country borders the Mediterranean Sea\\n        set<string> borders = countries.at(country);\\n        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\\n            cout << country << \" borders the Mediterranean Sea.\" << endl;\\n        } else {\\n            cout << country << \" does not border the Mediterranean Sea.\" << endl;\\n        }\\n    } else {\\n        cout << country << \" is not a recognized country.\" << endl;\\n    }\\n\\n    return 0;\\n}\\n\\nIn this program, we first define a map called countries that stores each country\\'s name as a key and a set of its surrounding seas as the value. We use a set to store each sea name so that we can easily check if a country borders a certain sea.\\n\\nWe then present the user with a prompt to enter the name of a country. We use a while loop to ensure that the user enters a valid country name. We check if the country is in the map, and if so, we retrieve its bordering seas from the map.\\n\\nFinally, we check if the Mediterranean Sea is among the set of borders. If it is, we inform the user that the country borders the Mediterranean Sea. If not, we inform the user that the country does not border the Mediterranean Sea.', 'rejected_response': 'Sure, here is the program using the C++11 algorithm \"cds::algorithm::GreaterEqual\":\\n#include <iostream>\\n#include <string>\\n#include <algorithm>\\n#include <vector>\\n#include <cctype>\\n\\nusing namespace std;\\n\\nint main() {\\n    string country;\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n    std::vector<string> vec;\\n    vec.push_back(country);\\n    size_t index = std::find_if(vec.begin(), vec.end(), [](const string& s) {\\n        return std::any_of(s.begin(), s.end(), [](const char& c) {\\n            return c == \\'}}\\';\\n    });\\n    if (index != vec.end()) {\\n        if ((*index)[0] == \\'M\\') {\\n            cout << \"Country is bordered by the Mediterranean Sea.\" << endl;\\n        } else {\\n            cout << \"Country does not border the Mediterranean Sea.\" << endl;\\n        }\\n    } else {\\n        cout << \"Country is not found.\" << endl;\\n    }\\n    return 0;\\n}', 'chosen_avg_rating': 5.0, 'rejected_avg_rating': 1.25, 'chosen_model': 'starchat'}\n",
            "['source', 'instruction', 'chosen_response', 'rejected_response', 'chosen_avg_rating', 'rejected_avg_rating', 'chosen_model']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_info = {\n",
        "    \"ultrafeedback\": {\n",
        "        \"hf_hub_url\": \"argilla/ultrafeedback-binarized-preferences\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"chosen\": \"chosen_response\",\n",
        "            \"rejected\": \"rejected_response\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"dataset_info.json\", \"w\") as f:\n",
        "    json.dump(dataset_info, f, indent=2)"
      ],
      "metadata": {
        "id": "0YSj2XnqKsCk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_args = {\n",
        "    \"stage\": \"sft\",\n",
        "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
        "    \"dataset\": \"ultrafeedback\",\n",
        "    \"dataset_dir\": \".\",\n",
        "    \"template\": \"alpaca\",\n",
        "    \"finetuning_type\": \"lora\",\n",
        "    \"lora_target\": \"q_proj,v_proj\",\n",
        "    \"output_dir\": \"llama2_sft_lora_ultrafeedback\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"max_samples\": 10000,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"logging_steps\": 10,\n",
        "    \"save_steps\": 1000,\n",
        "    \"eval_steps\": 200,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"max_grad_norm\": 0.3,\n",
        "    \"val_size\": 0.05,\n",
        "    \"preprocessing_num_workers\": 4,\n",
        "    \"quantization_bit\": 4\n",
        "}\n",
        "\n",
        "with open(\"sft_config.json\", \"w\") as f:\n",
        "    json.dump(sft_args, f, indent=2)"
      ],
      "metadata": {
        "id": "XRBbHmruKsCl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model - SFT"
      ],
      "metadata": {
        "id": "hcdy_oRf81hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli train sft_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bde32b-d4fd-4cf9-cbcc-84159a0fcb27",
        "id": "Hqm7YC6kKsCl"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-14 22:40:47.004610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 22:40:47.344937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 22:40:47.443142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 22:40:47.994861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 22:40:50.370044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "10/14/2024 22:40:59 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.\n",
            "10/14/2024 22:40:59 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "config.json: 100% 609/609 [00:00<00:00, 3.76MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 22:40:59,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 22:40:59,539 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 6.32MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 82.8MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 4.26MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.56MB/s]\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:01,908 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:01,908 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:01,908 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:01,908 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:01,908 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 22:41:03,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 22:41:03,069 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:03,303 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:03,303 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:03,303 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:03,303 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 22:41:03,303 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n",
            "10/14/2024 22:41:03 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.\n",
            "10/14/2024 22:41:03 - INFO - llamafactory.data.template - Add pad token: </s>\n",
            "10/14/2024 22:41:03 - INFO - llamafactory.data.loader - Loading dataset argilla/ultrafeedback-binarized-preferences...\n",
            "Converting format of dataset (num_proc=4): 100% 10000/10000 [00:01<00:00, 9011.57 examples/s]\n",
            "Running tokenizer on dataset (num_proc=4): 100% 10000/10000 [00:21<00:00, 467.01 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 835, 2799, 4080, 29901, 13, 6028, 366, 2436, 263, 315, 1817, 1824, 393, 9508, 29879, 278, 1404, 304, 3896, 278, 1024, 310, 263, 4234, 322, 12747, 565, 372, 28199, 278, 25620, 10800, 273, 14070, 29973, 2266, 29915, 29879, 777, 380, 4254, 775, 304, 1371, 366, 714, 29901, 13, 29937, 2856, 529, 23811, 29958, 13, 29937, 2856, 529, 1807, 29958, 13, 4746, 7397, 3659, 29936, 13, 524, 1667, 580, 426, 13, 1678, 1347, 4234, 29936, 13, 1678, 849, 9508, 1404, 363, 1881, 13, 1678, 11196, 3532, 376, 10399, 278, 1024, 310, 263, 4234, 29901, 12159, 13, 1678, 4670, 5099, 4234, 29936, 13, 1678, 849, 1423, 565, 4234, 28199, 278, 25620, 10800, 273, 14070, 13, 1678, 849, 518, 29907, 1817, 775, 29962, 13, 1678, 736, 29871, 29900, 29936, 13, 29913, 13, 13, 2277, 29937, 13291, 29901, 13, 2266, 29915, 29879, 263, 315, 1817, 1824, 393, 9508, 29879, 278, 1404, 304, 3896, 278, 1024, 310, 263, 4234, 322, 12747, 565, 372, 28199, 278, 25620, 10800, 273, 14070, 29901, 13, 13, 29937, 2856, 529, 23811, 29958, 13, 29937, 2856, 529, 1807, 29958, 13, 29937, 2856, 529, 842, 29958, 13, 29937, 2856, 529, 1958, 29958, 13, 29937, 2856, 529, 20567, 29958, 13, 13, 4746, 7397, 3659, 29936, 13, 13, 524, 1667, 580, 426, 13, 1678, 849, 3787, 10916, 322, 1009, 5139, 292, 409, 294, 297, 263, 2910, 13, 1678, 2910, 29966, 1807, 29892, 731, 29966, 1807, 6778, 10916, 29936, 13, 1678, 10916, 3366, 2499, 914, 423, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 1678, 10916, 3366, 16066, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 24636, 17368, 29908, 3400, 13, 1678, 10916, 3366, 10512, 29891, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 3253, 374, 2454, 14070, 29908, 3400, 13, 1678, 10916, 3366, 22995, 941, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 1678, 10916, 3366, 5592, 475, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 4178, 29880, 7716, 21091, 29908, 3400, 13, 1678, 10916, 3366, 29911, 348, 275, 423, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 13, 1678, 1347, 4234, 29936, 13, 1678, 11196, 3532, 376, 10399, 278, 1024, 310, 263, 4234, 29901, 12159, 13, 1678, 4670, 5099, 4234, 29936, 13, 13, 1678, 849, 1423, 565, 4234, 338, 297, 278, 2910, 13, 1678, 565, 313, 2798, 2722, 29889, 2886, 29898, 13509, 29897, 19216, 10916, 29889, 355, 3101, 426, 13, 4706, 849, 1423, 565, 4234, 28199, 278, 25620, 10800, 273, 14070, 13, 4706, 731, 29966, 1807, 29958, 28199, 353, 10916, 29889, 271, 29898, 13509, 416, 13, 4706, 565, 313, 29890, 20488, 29889, 2886, 703, 19302, 1524, 10800, 273, 14070, 1159, 19216, 28199, 29889, 355, 3101, 426, 13, 9651, 11196, 3532, 4234, 3532, 376, 28199, 278, 25620, 10800, 273, 14070, 1213, 3532, 18659, 29936, 13, 4706, 500, 1683, 426, 13, 9651, 11196, 3532, 4234, 3532, 376, 947, 451, 5139, 278, 25620, 10800, 273, 14070, 1213, 3532, 18659, 29936, 13, 4706, 500, 13, 1678, 500, 1683, 426, 13, 4706, 11196, 3532, 4234, 3532, 376, 338, 451, 263, 14831, 4234, 1213, 3532, 18659, 29936, 13, 1678, 500, 13, 13, 1678, 736, 29871, 29900, 29936, 13, 29913, 13, 13, 797, 445, 1824, 29892, 591, 937, 4529, 263, 2910, 2000, 10916, 393, 14422, 1269, 4234, 29915, 29879, 1024, 408, 263, 1820, 322, 263, 731, 310, 967, 18830, 409, 294, 408, 278, 995, 29889, 1334, 671, 263, 731, 304, 3787, 1269, 7205, 1024, 577, 393, 591, 508, 5948, 1423, 565, 263, 4234, 28199, 263, 3058, 7205, 29889, 13, 13, 4806, 769, 2198, 278, 1404, 411, 263, 9508, 304, 3896, 278, 1024, 310, 263, 4234, 29889, 1334, 671, 263, 1550, 2425, 304, 9801, 393, 278, 1404, 24395, 263, 2854, 4234, 1024, 29889, 1334, 1423, 565, 278, 4234, 338, 297, 278, 2910, 29892, 322, 565, 577, 29892, 591, 10563, 967, 5139, 292, 409, 294, 515, 278, 2910, 29889, 13, 13, 12881, 635, 29892, 591, 1423, 565, 278, 25620, 10800, 273, 14070, 338, 4249, 278, 731, 310, 28199, 29889, 960, 372, 338, 29892, 591, 1871, 278, 1404, 393, 278, 4234, 28199, 278, 25620, 10800, 273, 14070, 29889, 960, 451, 29892, 591, 1871, 278, 1404, 393, 278, 4234, 947, 451, 5139, 278, 25620, 10800, 273, 14070, 29889, 2]\n",
            "inputs:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            " ### Instruction:\n",
            "Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here's some starter code to help you out:\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "using namespace std;\n",
            "int main() {\n",
            "    string country;\n",
            "    // prompt user for input\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    // check if country borders the Mediterranean Sea\n",
            "    // [C++ code]\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "### Response:\n",
            " Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\n",
            "    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\n",
            "    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\n",
            "    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "\n",
            "    // check if country is in the map\n",
            "    if (countries.find(country)!= countries.end()) {\n",
            "        // check if country borders the Mediterranean Sea\n",
            "        set<string> borders = countries.at(country);\n",
            "        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\n",
            "            cout << country << \" borders the Mediterranean Sea.\" << endl;\n",
            "        } else {\n",
            "            cout << country << \" does not border the Mediterranean Sea.\" << endl;\n",
            "        }\n",
            "    } else {\n",
            "        cout << country << \" is not a recognized country.\" << endl;\n",
            "    }\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "In this program, we first define a map called countries that stores each country's name as a key and a set of its surrounding seas as the value. We use a set to store each sea name so that we can easily check if a country borders a certain sea.\n",
            "\n",
            "We then present the user with a prompt to enter the name of a country. We use a while loop to ensure that the user enters a valid country name. We check if the country is in the map, and if so, we retrieve its bordering seas from the map.\n",
            "\n",
            "Finally, we check if the Mediterranean Sea is among the set of borders. If it is, we inform the user that the country borders the Mediterranean Sea. If not, we inform the user that the country does not border the Mediterranean Sea.</s>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2266, 29915, 29879, 263, 315, 1817, 1824, 393, 9508, 29879, 278, 1404, 304, 3896, 278, 1024, 310, 263, 4234, 322, 12747, 565, 372, 28199, 278, 25620, 10800, 273, 14070, 29901, 13, 13, 29937, 2856, 529, 23811, 29958, 13, 29937, 2856, 529, 1807, 29958, 13, 29937, 2856, 529, 842, 29958, 13, 29937, 2856, 529, 1958, 29958, 13, 29937, 2856, 529, 20567, 29958, 13, 13, 4746, 7397, 3659, 29936, 13, 13, 524, 1667, 580, 426, 13, 1678, 849, 3787, 10916, 322, 1009, 5139, 292, 409, 294, 297, 263, 2910, 13, 1678, 2910, 29966, 1807, 29892, 731, 29966, 1807, 6778, 10916, 29936, 13, 1678, 10916, 3366, 2499, 914, 423, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 1678, 10916, 3366, 16066, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 24636, 17368, 29908, 3400, 13, 1678, 10916, 3366, 10512, 29891, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 3253, 374, 2454, 14070, 29908, 3400, 13, 1678, 10916, 3366, 22995, 941, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 1678, 10916, 3366, 5592, 475, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 4178, 29880, 7716, 21091, 29908, 3400, 13, 1678, 10916, 3366, 29911, 348, 275, 423, 3108, 353, 8853, 19302, 1524, 10800, 273, 14070, 613, 376, 29940, 2072, 11715, 17700, 29908, 3400, 13, 13, 1678, 1347, 4234, 29936, 13, 1678, 11196, 3532, 376, 10399, 278, 1024, 310, 263, 4234, 29901, 12159, 13, 1678, 4670, 5099, 4234, 29936, 13, 13, 1678, 849, 1423, 565, 4234, 338, 297, 278, 2910, 13, 1678, 565, 313, 2798, 2722, 29889, 2886, 29898, 13509, 29897, 19216, 10916, 29889, 355, 3101, 426, 13, 4706, 849, 1423, 565, 4234, 28199, 278, 25620, 10800, 273, 14070, 13, 4706, 731, 29966, 1807, 29958, 28199, 353, 10916, 29889, 271, 29898, 13509, 416, 13, 4706, 565, 313, 29890, 20488, 29889, 2886, 703, 19302, 1524, 10800, 273, 14070, 1159, 19216, 28199, 29889, 355, 3101, 426, 13, 9651, 11196, 3532, 4234, 3532, 376, 28199, 278, 25620, 10800, 273, 14070, 1213, 3532, 18659, 29936, 13, 4706, 500, 1683, 426, 13, 9651, 11196, 3532, 4234, 3532, 376, 947, 451, 5139, 278, 25620, 10800, 273, 14070, 1213, 3532, 18659, 29936, 13, 4706, 500, 13, 1678, 500, 1683, 426, 13, 4706, 11196, 3532, 4234, 3532, 376, 338, 451, 263, 14831, 4234, 1213, 3532, 18659, 29936, 13, 1678, 500, 13, 13, 1678, 736, 29871, 29900, 29936, 13, 29913, 13, 13, 797, 445, 1824, 29892, 591, 937, 4529, 263, 2910, 2000, 10916, 393, 14422, 1269, 4234, 29915, 29879, 1024, 408, 263, 1820, 322, 263, 731, 310, 967, 18830, 409, 294, 408, 278, 995, 29889, 1334, 671, 263, 731, 304, 3787, 1269, 7205, 1024, 577, 393, 591, 508, 5948, 1423, 565, 263, 4234, 28199, 263, 3058, 7205, 29889, 13, 13, 4806, 769, 2198, 278, 1404, 411, 263, 9508, 304, 3896, 278, 1024, 310, 263, 4234, 29889, 1334, 671, 263, 1550, 2425, 304, 9801, 393, 278, 1404, 24395, 263, 2854, 4234, 1024, 29889, 1334, 1423, 565, 278, 4234, 338, 297, 278, 2910, 29892, 322, 565, 577, 29892, 591, 10563, 967, 5139, 292, 409, 294, 515, 278, 2910, 29889, 13, 13, 12881, 635, 29892, 591, 1423, 565, 278, 25620, 10800, 273, 14070, 338, 4249, 278, 731, 310, 28199, 29889, 960, 372, 338, 29892, 591, 1871, 278, 1404, 393, 278, 4234, 28199, 278, 25620, 10800, 273, 14070, 29889, 960, 451, 29892, 591, 1871, 278, 1404, 393, 278, 4234, 947, 451, 5139, 278, 25620, 10800, 273, 14070, 29889, 2]\n",
            "labels:\n",
            "Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\n",
            "    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\n",
            "    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\n",
            "    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "\n",
            "    // check if country is in the map\n",
            "    if (countries.find(country)!= countries.end()) {\n",
            "        // check if country borders the Mediterranean Sea\n",
            "        set<string> borders = countries.at(country);\n",
            "        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\n",
            "            cout << country << \" borders the Mediterranean Sea.\" << endl;\n",
            "        } else {\n",
            "            cout << country << \" does not border the Mediterranean Sea.\" << endl;\n",
            "        }\n",
            "    } else {\n",
            "        cout << country << \" is not a recognized country.\" << endl;\n",
            "    }\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "In this program, we first define a map called countries that stores each country's name as a key and a set of its surrounding seas as the value. We use a set to store each sea name so that we can easily check if a country borders a certain sea.\n",
            "\n",
            "We then present the user with a prompt to enter the name of a country. We use a while loop to ensure that the user enters a valid country name. We check if the country is in the map, and if so, we retrieve its bordering seas from the map.\n",
            "\n",
            "Finally, we check if the Mediterranean Sea is among the set of borders. If it is, we inform the user that the country borders the Mediterranean Sea. If not, we inform the user that the country does not border the Mediterranean Sea.</s>\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 22:41:30,132 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 22:41:30,133 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "10/14/2024 22:41:30 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "10/14/2024 22:41:30 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 105MB/s]\n",
            "[INFO|modeling_utils.py:3678] 2024-10-14 22:41:31,289 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/9.98G [00:00<01:31, 109MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/9.98G [00:00<00:42, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 105M/9.98G [00:00<00:33, 298MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 147M/9.98G [00:00<00:32, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 189M/9.98G [00:00<00:31, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 231M/9.98G [00:00<00:29, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 273M/9.98G [00:01<00:38, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 304M/9.98G [00:01<00:37, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 346M/9.98G [00:01<00:33, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 388M/9.98G [00:01<00:33, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 430M/9.98G [00:01<00:32, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 461M/9.98G [00:01<00:33, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.98G [00:01<00:34, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 524M/9.98G [00:01<00:35, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 556M/9.98G [00:02<00:37, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 587M/9.98G [00:02<01:31, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.98G [00:02<01:15, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 650M/9.98G [00:03<01:04, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.98G [00:03<00:55, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 713M/9.98G [00:03<00:50, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 744M/9.98G [00:03<00:46, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 776M/9.98G [00:03<00:43, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.98G [00:03<00:42, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 839M/9.98G [00:03<00:42, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.98G [00:03<00:41, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 902M/9.98G [00:04<00:41, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.98G [00:04<00:37, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 965M/9.98G [00:04<00:38, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 996M/9.98G [00:04<00:40, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.03G/9.98G [00:04<00:38, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.06G/9.98G [00:04<00:36, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.09G/9.98G [00:04<00:38, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.98G [00:05<00:36, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.16G/9.98G [00:05<00:32, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.20G/9.98G [00:05<00:31, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.24G/9.98G [00:05<00:28, 304MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.98G [00:05<00:29, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.31G/9.98G [00:05<00:28, 304MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.34G/9.98G [00:05<00:28, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.38G/9.98G [00:05<00:26, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.98G [00:05<00:26, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.47G/9.98G [00:06<00:26, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.51G/9.98G [00:06<00:25, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.55G/9.98G [00:06<00:25, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.59G/9.98G [00:07<01:29, 93.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.64G/9.98G [00:07<01:09, 120MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.67G/9.98G [00:07<00:59, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.98G [00:07<00:47, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.98G [00:08<00:40, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.79G/9.98G [00:08<00:35, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.98G [00:08<00:31, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.98G [00:08<00:29, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.92G/9.98G [00:08<00:27, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.98G [00:08<00:27, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.98G [00:08<00:26, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.04G/9.98G [00:08<00:25, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:10<01:22, 95.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.14G/9.98G [00:10<00:59, 132MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.18G/9.98G [00:10<00:47, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.22G/9.98G [00:10<00:42, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.98G [00:10<00:39, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.30G/9.98G [00:11<01:36, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.33G/9.98G [00:14<03:48, 33.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:14<02:28, 51.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.98G [00:14<01:49, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.45G/9.98G [00:14<01:29, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.49G/9.98G [00:14<01:15, 99.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.52G/9.98G [00:14<01:01, 122MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.55G/9.98G [00:15<00:51, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.58G/9.98G [00:15<00:46, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.61G/9.98G [00:15<00:42, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.64G/9.98G [00:15<00:38, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.67G/9.98G [00:15<00:35, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.98G [00:15<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.74G/9.98G [00:15<00:34, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.77G/9.98G [00:16<00:33, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.98G [00:16<00:32, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.83G/9.98G [00:16<00:29, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.86G/9.98G [00:16<00:31, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.89G/9.98G [00:16<00:30, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.93G/9.98G [00:16<00:29, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.97G/9.98G [00:16<00:27, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.00G/9.98G [00:16<00:27, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.04G/9.98G [00:17<00:25, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.07G/9.98G [00:17<00:28, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.98G [00:17<00:28, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:17<00:28, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.98G [00:17<00:28, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.21G/9.98G [00:17<00:25, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.24G/9.98G [00:17<00:26, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.27G/9.98G [00:18<00:26, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.98G [00:18<00:27, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.33G/9.98G [00:19<01:13, 89.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.36G/9.98G [00:19<01:21, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.38G/9.98G [00:20<02:24, 45.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.40G/9.98G [00:21<03:10, 34.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.98G [00:22<03:53, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.45G/9.98G [00:22<02:35, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.48G/9.98G [00:22<01:50, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.51G/9.98G [00:23<01:23, 77.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.54G/9.98G [00:23<01:03, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.58G/9.98G [00:23<00:50, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.98G [00:23<00:38, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.66G/9.98G [00:23<00:31, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.98G [00:23<00:27, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.73G/9.98G [00:24<01:19, 78.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.77G/9.98G [00:24<00:57, 107MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.98G [00:26<01:40, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.85G/9.98G [00:26<01:12, 85.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.88G/9.98G [00:26<00:58, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.91G/9.98G [00:26<00:49, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.94G/9.98G [00:26<00:40, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.97G/9.98G [00:28<02:27, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.03G/9.98G [00:28<01:32, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.07G/9.98G [00:28<01:07, 86.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.98G [00:29<00:56, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.13G/9.98G [00:29<00:48, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.16G/9.98G [00:29<00:42, 137MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.19G/9.98G [00:29<00:37, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.23G/9.98G [00:29<00:35, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.26G/9.98G [00:30<01:23, 68.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.29G/9.98G [00:30<01:05, 86.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.32G/9.98G [00:30<00:52, 109MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.35G/9.98G [00:31<00:43, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.38G/9.98G [00:31<00:39, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.41G/9.98G [00:31<00:35, 156MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.44G/9.98G [00:31<00:34, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.46G/9.98G [00:32<00:56, 97.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.98G [00:32<00:43, 127MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.51G/9.98G [00:32<00:42, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.53G/9.98G [00:32<00:43, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.98G [00:32<00:38, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.58G/9.98G [00:32<00:32, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.98G [00:32<00:37, 142MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.62G/9.98G [00:33<00:36, 145MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.65G/9.98G [00:33<00:34, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.98G [00:33<00:32, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.69G/9.98G [00:33<00:31, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.71G/9.98G [00:33<00:30, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.73G/9.98G [00:33<00:29, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.75G/9.98G [00:33<00:40, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.78G/9.98G [00:33<00:32, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.81G/9.98G [00:34<00:28, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.83G/9.98G [00:34<00:27, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.85G/9.98G [00:34<00:28, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.89G/9.98G [00:34<00:26, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.92G/9.98G [00:34<00:24, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.98G [00:35<01:02, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.97G/9.98G [00:38<03:58, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.02G/9.98G [00:39<02:12, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.08G/9.98G [00:39<01:24, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.98G [00:39<01:07, 72.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.14G/9.98G [00:39<00:54, 88.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.98G [00:39<00:40, 117MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.21G/9.98G [00:39<00:34, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.25G/9.98G [00:39<00:27, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.98G [00:39<00:24, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.32G/9.98G [00:40<00:22, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.98G [00:40<00:21, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.38G/9.98G [00:40<00:20, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.98G [00:40<00:19, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.44G/9.98G [00:40<00:19, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.47G/9.98G [00:40<00:19, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.51G/9.98G [00:40<00:18, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.54G/9.98G [00:40<00:17, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.57G/9.98G [00:40<00:17, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.61G/9.98G [00:41<00:17, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.65G/9.98G [00:41<00:15, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.68G/9.98G [00:41<00:15, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.71G/9.98G [00:41<00:15, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.76G/9.98G [00:41<00:14, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.98G [00:41<00:14, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.82G/9.98G [00:41<00:14, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.85G/9.98G [00:41<00:14, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.89G/9.98G [00:42<00:14, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.93G/9.98G [00:42<00:13, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.97G/9.98G [00:42<00:14, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.98G [00:42<00:14, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.03G/9.98G [00:42<00:14, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.06G/9.98G [00:42<00:15, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:42<00:14, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.12G/9.98G [00:43<00:14, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.16G/9.98G [00:43<00:16, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.20G/9.98G [00:43<00:14, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.23G/9.98G [00:43<00:15, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.98G [00:43<00:13, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.30G/9.98G [00:43<00:14, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.33G/9.98G [00:43<00:14, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.36G/9.98G [00:43<00:13, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.98G [00:44<00:13, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.43G/9.98G [00:44<00:13, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.98G [00:44<00:14, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.49G/9.98G [00:44<00:15, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.52G/9.98G [00:44<00:15, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.55G/9.98G [00:44<00:14, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.98G [00:44<00:14, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.62G/9.98G [00:45<00:14, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.65G/9.98G [00:45<00:14, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.68G/9.98G [00:45<00:14, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.71G/9.98G [00:45<00:14, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.74G/9.98G [00:45<00:13, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.98G [00:45<00:14, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.81G/9.98G [00:45<00:14, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.98G [00:46<00:13, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.87G/9.98G [00:46<00:14, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.98G [00:46<00:13, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.93G/9.98G [00:46<00:13, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.98G [00:46<00:14, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.99G/9.98G [00:46<00:13, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.03G/9.98G [00:46<00:13, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.06G/9.98G [00:47<00:13, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.09G/9.98G [00:47<00:12, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.12G/9.98G [00:47<00:12, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.15G/9.98G [00:47<00:12, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.18G/9.98G [00:47<00:11, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.21G/9.98G [00:47<00:12, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.98G [00:47<00:12, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.28G/9.98G [00:47<00:11, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.31G/9.98G [00:48<00:10, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.98G [00:48<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.37G/9.98G [00:48<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.40G/9.98G [00:48<00:10, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.43G/9.98G [00:48<00:10, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.98G [00:48<00:09, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.50G/9.98G [00:48<00:10, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.53G/9.98G [00:48<00:09, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.56G/9.98G [00:49<00:09, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.59G/9.98G [00:49<00:09, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.62G/9.98G [00:49<00:09, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.65G/9.98G [00:49<00:09, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.70G/9.98G [00:49<00:07, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.73G/9.98G [00:49<00:07, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.78G/9.98G [00:49<00:06, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.82G/9.98G [00:49<00:07, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:50<00:09, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.89G/9.98G [00:55<01:35, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.93G/9.98G [00:55<01:03, 32.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.97G/9.98G [00:55<00:44, 45.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.98G [00:55<00:34, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.04G/9.98G [00:55<00:25, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.08G/9.98G [00:55<00:18, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.12G/9.98G [00:56<00:15, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.16G/9.98G [00:56<00:11, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.20G/9.98G [00:56<00:09, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.23G/9.98G [00:56<00:08, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.26G/9.98G [00:56<00:08, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.29G/9.98G [00:56<00:07, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.33G/9.98G [00:56<00:07, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.36G/9.98G [00:56<00:06, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.39G/9.98G [00:57<00:06, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.42G/9.98G [00:57<00:06, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.45G/9.98G [00:57<00:06, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.48G/9.98G [00:57<00:06, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.51G/9.98G [00:57<00:05, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.55G/9.98G [00:57<00:05, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.58G/9.98G [00:57<00:05, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.61G/9.98G [00:57<00:05, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.98G [00:58<00:05, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.98G [00:58<00:05, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.70G/9.98G [00:58<00:05, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.73G/9.98G [00:58<00:04, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.77G/9.98G [00:58<00:04, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.80G/9.98G [00:58<00:04, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.83G/9.98G [00:58<00:04, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.86G/9.98G [00:58<00:04, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.89G/9.98G [00:59<00:04, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.92G/9.98G [00:59<00:04, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.95G/9.98G [00:59<00:04, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.99G/9.98G [00:59<00:03, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:59<00:04, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.05G/9.98G [00:59<00:04, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:59<00:04, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.11G/9.98G [01:00<00:04, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.98G [01:00<00:03, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.18G/9.98G [01:00<00:03, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.21G/9.98G [01:00<00:03, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.24G/9.98G [01:00<00:03, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.27G/9.98G [01:00<00:02, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.30G/9.98G [01:00<00:02, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.33G/9.98G [01:01<00:02, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.36G/9.98G [01:01<00:02, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.40G/9.98G [01:01<00:02, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.43G/9.98G [01:01<00:02, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.46G/9.98G [01:01<00:02, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.49G/9.98G [01:01<00:01, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.52G/9.98G [01:01<00:01, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.55G/9.98G [01:01<00:01, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.58G/9.98G [01:02<00:01, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.62G/9.98G [01:02<00:01, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.65G/9.98G [01:02<00:01, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.69G/9.98G [01:02<00:01, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.73G/9.98G [01:02<00:00, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.76G/9.98G [01:02<00:00, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.79G/9.98G [01:02<00:00, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.83G/9.98G [01:05<00:04, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.98G [01:05<00:01, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.93G/9.98G [01:05<00:00, 86.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [01:05<00:00, 151MB/s]\n",
            "Downloading shards:  50% 1/2 [01:06<01:06, 66.16s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 52.4M/3.50G [00:00<00:08, 420MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 105M/3.50G [00:00<00:10, 318MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 147M/3.50G [00:00<00:10, 312MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 189M/3.50G [00:05<02:30, 22.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 220M/3.50G [00:05<01:50, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 252M/3.50G [00:05<01:20, 40.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 294M/3.50G [00:05<00:55, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 325M/3.50G [00:05<00:44, 70.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 357M/3.50G [00:06<00:34, 90.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 388M/3.50G [00:06<00:27, 114MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 430M/3.50G [00:06<00:20, 147MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 472M/3.50G [00:06<00:16, 178MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 503M/3.50G [00:06<00:15, 195MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 535M/3.50G [00:06<00:14, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 566M/3.50G [00:06<00:13, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 598M/3.50G [00:06<00:12, 228MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 629M/3.50G [00:07<00:12, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 661M/3.50G [00:07<00:12, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 692M/3.50G [00:07<00:12, 231MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 724M/3.50G [00:07<00:12, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 755M/3.50G [00:07<00:11, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 786M/3.50G [00:07<00:11, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 818M/3.50G [00:07<00:11, 230MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 849M/3.50G [00:08<00:11, 238MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 881M/3.50G [00:08<00:11, 232MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 912M/3.50G [00:08<00:11, 231MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 944M/3.50G [00:08<00:11, 227MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 975M/3.50G [00:08<00:11, 228MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.01G/3.50G [00:08<00:11, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.04G/3.50G [00:08<00:10, 234MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.07G/3.50G [00:08<00:10, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.10G/3.50G [00:09<00:10, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.13G/3.50G [00:09<00:09, 238MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.16G/3.50G [00:09<00:09, 240MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.20G/3.50G [00:09<00:09, 252MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.23G/3.50G [00:09<00:09, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.26G/3.50G [00:09<00:09, 243MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.29G/3.50G [00:09<00:09, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.32G/3.50G [00:09<00:08, 250MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.35G/3.50G [00:10<00:08, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.38G/3.50G [00:10<00:08, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.42G/3.50G [00:10<00:08, 240MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.45G/3.50G [00:10<00:08, 245MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.48G/3.50G [00:10<00:07, 257MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.51G/3.50G [00:10<00:08, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.54G/3.50G [00:10<00:07, 245MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.57G/3.50G [00:11<00:07, 257MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.60G/3.50G [00:11<00:07, 264MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.64G/3.50G [00:11<00:07, 255MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.67G/3.50G [00:11<00:08, 225MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.70G/3.50G [00:11<00:07, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.73G/3.50G [00:11<00:08, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.77G/3.50G [00:11<00:06, 256MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.82G/3.50G [00:11<00:05, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.87G/3.50G [00:12<00:05, 288MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.90G/3.50G [00:12<00:05, 283MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.93G/3.50G [00:12<00:13, 115MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.97G/3.50G [00:13<00:12, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.00G/3.50G [00:13<00:10, 146MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.04G/3.50G [00:13<00:08, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.08G/3.50G [00:13<00:07, 191MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.11G/3.50G [00:13<00:06, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.14G/3.50G [00:13<00:06, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.17G/3.50G [00:13<00:05, 225MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.20G/3.50G [00:14<00:05, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:14<00:05, 240MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.26G/3.50G [00:14<00:05, 243MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.30G/3.50G [00:14<00:04, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 2.33G/3.50G [00:14<00:04, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 2.36G/3.50G [00:14<00:07, 160MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 2.39G/3.50G [00:15<00:06, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 2.42G/3.50G [00:15<00:05, 184MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 2.45G/3.50G [00:15<00:05, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 2.50G/3.50G [00:15<00:04, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 2.53G/3.50G [00:15<00:05, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 2.57G/3.50G [00:15<00:04, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.61G/3.50G [00:15<00:03, 254MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.65G/3.50G [00:16<00:02, 288MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.69G/3.50G [00:16<00:03, 267MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.73G/3.50G [00:16<00:02, 263MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.76G/3.50G [00:16<00:03, 243MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.80G/3.50G [00:16<00:02, 253MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.83G/3.50G [00:16<00:02, 264MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.86G/3.50G [00:16<00:02, 250MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.89G/3.50G [00:17<00:02, 262MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.93G/3.50G [00:17<00:02, 269MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.96G/3.50G [00:17<00:02, 253MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.99G/3.50G [00:17<00:02, 238MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:17<00:01, 259MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.06G/3.50G [00:17<00:01, 254MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.09G/3.50G [00:17<00:01, 253MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 3.12G/3.50G [00:18<00:01, 240MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:18<00:03, 90.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 3.19G/3.50G [00:18<00:02, 114MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 3.22G/3.50G [00:19<00:02, 135MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 3.25G/3.50G [00:19<00:01, 159MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 3.28G/3.50G [00:25<00:14, 15.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 3.31G/3.50G [00:25<00:09, 20.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:26<00:03, 34.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 3.41G/3.50G [00:26<00:01, 47.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 3.44G/3.50G [00:26<00:01, 57.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:26<00:00, 131MB/s] \n",
            "Downloading shards: 100% 2/2 [01:33<00:00, 46.67s/it]\n",
            "[INFO|modeling_utils.py:1606] 2024-10-14 22:43:04,631 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1038] 2024-10-14 22:43:04,646 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [01:07<00:00, 33.79s/it]\n",
            "[INFO|modeling_utils.py:4507] 2024-10-14 22:44:13,363 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-10-14 22:44:13,363 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.05MB/s]\n",
            "[INFO|configuration_utils.py:993] 2024-10-14 22:44:13,840 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-10-14 22:44:13,840 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "10/14/2024 22:44:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "10/14/2024 22:44:14 - INFO - llamafactory.model.loader - all params: 6,738,415,616\n",
            "[INFO|trainer.py:3819] 2024-10-14 22:44:14,583 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3821] 2024-10-14 22:44:14,583 >>   Num examples = 500\n",
            "[INFO|trainer.py:3824] 2024-10-14 22:44:14,583 >>   Batch size = 8\n",
            "[WARNING|logging.py:328] 2024-10-14 22:44:15,537 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "100% 63/63 [06:35<00:00,  6.27s/it]\n",
            "***** eval metrics *****\n",
            "  eval_loss                   =     0.8605\n",
            "  eval_model_preparation_time =     0.0015\n",
            "  eval_runtime                = 0:06:41.13\n",
            "  eval_samples_per_second     =      1.246\n",
            "  eval_steps_per_second       =      0.157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsxqa503UR77",
        "outputId": "820223a6-8b8e-4cc8-d8fc-9f78797d2c82"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-14 23:27:22.854300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 23:27:22.875150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 23:27:22.881419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 23:27:22.896689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 23:27:24.169357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n",
            "Running on public URL: https://048df952514f4d1086.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-10-14 23:28:30.315412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 23:28:30.349541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 23:28:30.359889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 23:28:30.382244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 23:28:31.973481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/14/2024 23:28:36 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 23:28:36,824 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 23:28:36,827 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:37,066 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:37,066 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:37,066 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:37,066 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:37,066 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 23:28:38,811 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 23:28:38,811 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:39,052 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:39,052 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:39,052 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:39,052 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-14 23:28:39,052 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "10/14/2024 23:28:39 - WARNING - llamafactory.model.loader - Processor was not found: 'GemmaConfig' object has no attribute 'vision_config'.\n",
            "10/14/2024 23:28:39 - INFO - llamafactory.data.loader - Loading dataset argilla/ultrafeedback-binarized-preferences...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 3611, 692, 5598, 476, 585, 1641, 2733, 674, 73815, 573, 2425, 577, 3343, 573, 1503, 576, 476, 3170, 578, 19483, 1013, 665, 31127, 573, 35521, 9489, 235336, 5698, 235303, 235256, 1009, 29792, 3409, 577, 1707, 692, 921, 235292, 108, 235345, 1915, 968, 15874, 235313, 108, 235345, 1915, 968, 1973, 235313, 108, 1442, 9327, 6015, 235289, 108, 635, 1872, 645, 612, 108, 141, 1973, 3170, 235289, 108, 141, 634, 18335, 2425, 604, 3772, 108, 141, 6117, 3245, 664, 9836, 573, 1503, 576, 476, 3170, 235292, 15460, 108, 141, 3578, 5746, 3170, 235289, 108, 141, 634, 2701, 1013, 3170, 31127, 573, 35521, 9489, 108, 141, 634, 892, 235288, 1641, 3409, 235307, 108, 141, 773, 235248, 235276, 235289, 108, 235270, 107, 108, 106, 2516, 108, 4858, 235303, 235256, 476, 585, 1641, 2733, 674, 73815, 573, 2425, 577, 3343, 573, 1503, 576, 476, 3170, 578, 19483, 1013, 665, 31127, 573, 35521, 9489, 235292, 109, 235345, 1915, 968, 15874, 235313, 108, 235345, 1915, 968, 1973, 235313, 108, 235345, 1915, 968, 881, 235313, 108, 235345, 1915, 968, 2700, 235313, 108, 235345, 1915, 968, 28861, 235313, 109, 1442, 9327, 6015, 235289, 109, 635, 1872, 645, 612, 108, 141, 634, 4659, 5605, 578, 1024, 117054, 28318, 575, 476, 4403, 108, 141, 2700, 235322, 1973, 235269, 1142, 235322, 1973, 2492, 5605, 235289, 108, 141, 54025, 3681, 184626, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 108, 141, 54025, 3681, 21456, 4437, 589, 19946, 185073, 9489, 824, 664, 12023, 18769, 66553, 108, 141, 54025, 3681, 39359, 4437, 589, 19946, 185073, 9489, 824, 664, 80443, 5855, 9489, 66553, 108, 141, 54025, 3681, 94441, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 108, 141, 54025, 3681, 46297, 4437, 589, 19946, 185073, 9489, 824, 664, 75300, 17036, 66553, 108, 141, 54025, 3681, 184455, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 109, 141, 1973, 3170, 235289, 108, 141, 6117, 3245, 664, 9836, 573, 1503, 576, 476, 3170, 235292, 15460, 108, 141, 3578, 5746, 3170, 235289, 109, 141, 634, 2701, 1013, 3170, 603, 575, 573, 4403, 108, 141, 648, 591, 54025, 235265, 3655, 235278, 11857, 104312, 5605, 235265, 615, 2604, 612, 108, 145, 634, 2701, 1013, 3170, 31127, 573, 35521, 9489, 108, 145, 881, 235322, 1973, 235313, 31127, 589, 5605, 235265, 482, 235278, 11857, 594, 108, 145, 648, 591, 171976, 235265, 3655, 885, 185073, 9489, 1388, 10902, 31127, 235265, 615, 2604, 612, 108, 149, 6117, 3245, 3170, 3245, 664, 31127, 573, 35521, 9489, 1464, 3245, 13556, 235289, 108, 145, 235270, 1354, 612, 108, 149, 6117, 3245, 3170, 3245, 664, 1721, 780, 9994, 573, 35521, 9489, 1464, 3245, 13556, 235289, 108, 145, 235270, 108, 141, 235270, 1354, 612, 108, 145, 6117, 3245, 3170, 3245, 664, 603, 780, 476, 14712, 3170, 1464, 3245, 13556, 235289, 108, 141, 235270, 109, 141, 773, 235248, 235276, 235289, 108, 235270, 109, 886, 736, 2733, 235269, 783, 1370, 11810, 476, 4403, 3151, 5605, 674, 13494, 1853, 3170, 235303, 235256, 1503, 685, 476, 2621, 578, 476, 1142, 576, 1277, 16531, 28318, 685, 573, 1618, 235265, 1448, 1281, 476, 1142, 577, 1]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here's some starter code to help you out:\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "using namespace std;\n",
            "int main() {\n",
            "    string country;\n",
            "    // prompt user for input\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    // check if country borders the Mediterranean Sea\n",
            "    // [C++ code]\n",
            "    return 0;\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\n",
            "    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\n",
            "    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\n",
            "    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "\n",
            "    // check if country is in the map\n",
            "    if (countries.find(country)!= countries.end()) {\n",
            "        // check if country borders the Mediterranean Sea\n",
            "        set<string> borders = countries.at(country);\n",
            "        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\n",
            "            cout << country << \" borders the Mediterranean Sea.\" << endl;\n",
            "        } else {\n",
            "            cout << country << \" does not border the Mediterranean Sea.\" << endl;\n",
            "        }\n",
            "    } else {\n",
            "        cout << country << \" is not a recognized country.\" << endl;\n",
            "    }\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "In this program, we first define a map called countries that stores each country's name as a key and a set of its surrounding seas as the value. We use a set to<eos>\n",
            "label_ids:\n",
            "[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4858, 235303, 235256, 476, 585, 1641, 2733, 674, 73815, 573, 2425, 577, 3343, 573, 1503, 576, 476, 3170, 578, 19483, 1013, 665, 31127, 573, 35521, 9489, 235292, 109, 235345, 1915, 968, 15874, 235313, 108, 235345, 1915, 968, 1973, 235313, 108, 235345, 1915, 968, 881, 235313, 108, 235345, 1915, 968, 2700, 235313, 108, 235345, 1915, 968, 28861, 235313, 109, 1442, 9327, 6015, 235289, 109, 635, 1872, 645, 612, 108, 141, 634, 4659, 5605, 578, 1024, 117054, 28318, 575, 476, 4403, 108, 141, 2700, 235322, 1973, 235269, 1142, 235322, 1973, 2492, 5605, 235289, 108, 141, 54025, 3681, 184626, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 108, 141, 54025, 3681, 21456, 4437, 589, 19946, 185073, 9489, 824, 664, 12023, 18769, 66553, 108, 141, 54025, 3681, 39359, 4437, 589, 19946, 185073, 9489, 824, 664, 80443, 5855, 9489, 66553, 108, 141, 54025, 3681, 94441, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 108, 141, 54025, 3681, 46297, 4437, 589, 19946, 185073, 9489, 824, 664, 75300, 17036, 66553, 108, 141, 54025, 3681, 184455, 4437, 589, 19946, 185073, 9489, 824, 664, 17870, 10742, 14161, 66553, 109, 141, 1973, 3170, 235289, 108, 141, 6117, 3245, 664, 9836, 573, 1503, 576, 476, 3170, 235292, 15460, 108, 141, 3578, 5746, 3170, 235289, 109, 141, 634, 2701, 1013, 3170, 603, 575, 573, 4403, 108, 141, 648, 591, 54025, 235265, 3655, 235278, 11857, 104312, 5605, 235265, 615, 2604, 612, 108, 145, 634, 2701, 1013, 3170, 31127, 573, 35521, 9489, 108, 145, 881, 235322, 1973, 235313, 31127, 589, 5605, 235265, 482, 235278, 11857, 594, 108, 145, 648, 591, 171976, 235265, 3655, 885, 185073, 9489, 1388, 10902, 31127, 235265, 615, 2604, 612, 108, 149, 6117, 3245, 3170, 3245, 664, 31127, 573, 35521, 9489, 1464, 3245, 13556, 235289, 108, 145, 235270, 1354, 612, 108, 149, 6117, 3245, 3170, 3245, 664, 1721, 780, 9994, 573, 35521, 9489, 1464, 3245, 13556, 235289, 108, 145, 235270, 108, 141, 235270, 1354, 612, 108, 145, 6117, 3245, 3170, 3245, 664, 603, 780, 476, 14712, 3170, 1464, 3245, 13556, 235289, 108, 141, 235270, 109, 141, 773, 235248, 235276, 235289, 108, 235270, 109, 886, 736, 2733, 235269, 783, 1370, 11810, 476, 4403, 3151, 5605, 674, 13494, 1853, 3170, 235303, 235256, 1503, 685, 476, 2621, 578, 476, 1142, 576, 1277, 16531, 28318, 685, 573, 1618, 235265, 1448, 1281, 476, 1142, 577, 1]\n",
            "labels:\n",
            "<eos>Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\n",
            "    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\n",
            "    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\n",
            "    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "\n",
            "    // check if country is in the map\n",
            "    if (countries.find(country)!= countries.end()) {\n",
            "        // check if country borders the Mediterranean Sea\n",
            "        set<string> borders = countries.at(country);\n",
            "        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\n",
            "            cout << country << \" borders the Mediterranean Sea.\" << endl;\n",
            "        } else {\n",
            "            cout << country << \" does not border the Mediterranean Sea.\" << endl;\n",
            "        }\n",
            "    } else {\n",
            "        cout << country << \" is not a recognized country.\" << endl;\n",
            "    }\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "In this program, we first define a map called countries that stores each country's name as a key and a set of its surrounding seas as the value. We use a set to<eos>\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 23:28:44,627 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 23:28:44,629 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3678] 2024-10-14 23:28:44,715 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1606] 2024-10-14 23:28:44,720 >> Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1038] 2024-10-14 23:28:44,722 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-10-14 23:28:44,726 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:14<00:00,  7.25s/it]\n",
            "[INFO|modeling_utils.py:4507] 2024-10-14 23:28:59,589 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-10-14 23:28:59,589 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-10-14 23:28:59,834 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-10-14 23:28:59,834 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "10/14/2024 23:28:59 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "10/14/2024 23:28:59 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "10/14/2024 23:28:59 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "10/14/2024 23:28:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "10/14/2024 23:28:59 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,v_proj,gate_proj,k_proj,up_proj,down_proj\n",
            "10/14/2024 23:29:00 - INFO - llamafactory.model.loader - trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|trainer.py:648] 2024-10-14 23:29:00,498 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2134] 2024-10-14 23:29:00,917 >> ***** Running training *****\n",
            "[INFO|trainer.py:2135] 2024-10-14 23:29:00,917 >>   Num examples = 10,000\n",
            "[INFO|trainer.py:2136] 2024-10-14 23:29:00,917 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2137] 2024-10-14 23:29:00,917 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2140] 2024-10-14 23:29:00,917 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2141] 2024-10-14 23:29:00,917 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2142] 2024-10-14 23:29:00,917 >>   Total optimization steps = 625\n",
            "[INFO|trainer.py:2143] 2024-10-14 23:29:00,920 >>   Number of trainable parameters = 9,805,824\n",
            "  1% 5/625 [00:42<1:26:00,  8.32s/it]10/14/2024 23:29:43 - INFO - llamafactory.train.callbacks - {'loss': 1.3915, 'learning_rate': 1.9997e-04, 'epoch': 0.01, 'throughput': 895.18}\n",
            "{'loss': 1.3915, 'grad_norm': 0.7978965640068054, 'learning_rate': 0.00019996841892833, 'epoch': 0.01, 'num_input_tokens_seen': 38384}\n",
            "  2% 10/625 [01:26<1:27:30,  8.54s/it]10/14/2024 23:30:26 - INFO - llamafactory.train.callbacks - {'loss': 1.2352, 'learning_rate': 1.9987e-04, 'epoch': 0.02, 'throughput': 893.43}\n",
            "{'loss': 1.2352, 'grad_norm': 0.7597867846488953, 'learning_rate': 0.00019987369566060176, 'epoch': 0.02, 'num_input_tokens_seen': 76848}\n",
            "  2% 15/625 [02:10<1:29:41,  8.82s/it]10/14/2024 23:31:11 - INFO - llamafactory.train.callbacks - {'loss': 1.1307, 'learning_rate': 1.9972e-04, 'epoch': 0.02, 'throughput': 877.84}\n",
            "{'loss': 1.1307, 'grad_norm': 0.927596926689148, 'learning_rate': 0.0001997158900260614, 'epoch': 0.02, 'num_input_tokens_seen': 114848}\n",
            "  3% 20/625 [02:53<1:25:53,  8.52s/it]10/14/2024 23:31:54 - INFO - llamafactory.train.callbacks - {'loss': 1.2887, 'learning_rate': 1.9950e-04, 'epoch': 0.03, 'throughput': 872.45}\n",
            "{'loss': 1.2887, 'grad_norm': 0.7752978205680847, 'learning_rate': 0.00019949510169813003, 'epoch': 0.03, 'num_input_tokens_seen': 151664}\n",
            "  4% 25/625 [03:38<1:28:20,  8.83s/it]10/14/2024 23:32:38 - INFO - llamafactory.train.callbacks - {'loss': 1.2031, 'learning_rate': 1.9921e-04, 'epoch': 0.04, 'throughput': 867.45}\n",
            "{'loss': 1.2031, 'grad_norm': 0.6363417506217957, 'learning_rate': 0.0001992114701314478, 'epoch': 0.04, 'num_input_tokens_seen': 189168}\n",
            "  5% 30/625 [04:21<1:25:25,  8.61s/it]10/14/2024 23:33:22 - INFO - llamafactory.train.callbacks - {'loss': 1.2112, 'learning_rate': 1.9887e-04, 'epoch': 0.05, 'throughput': 864.5}\n",
            "{'loss': 1.2112, 'grad_norm': 0.6260274648666382, 'learning_rate': 0.0001988651744737914, 'epoch': 0.05, 'num_input_tokens_seen': 226240}\n",
            "  6% 35/625 [05:07<1:29:15,  9.08s/it]10/14/2024 23:34:08 - INFO - llamafactory.train.callbacks - {'loss': 1.2254, 'learning_rate': 1.9846e-04, 'epoch': 0.06, 'throughput': 864.03}\n",
            "{'loss': 1.2254, 'grad_norm': 0.5318862795829773, 'learning_rate': 0.00019845643345292054, 'epoch': 0.06, 'num_input_tokens_seen': 265632}\n",
            "  6% 40/625 [05:53<1:31:01,  9.34s/it]10/14/2024 23:34:54 - INFO - llamafactory.train.callbacks - {'loss': 1.1615, 'learning_rate': 1.9799e-04, 'epoch': 0.06, 'throughput': 862.09}\n",
            "{'loss': 1.1615, 'grad_norm': 0.5773426294326782, 'learning_rate': 0.0001979855052384247, 'epoch': 0.06, 'num_input_tokens_seen': 304720}\n",
            "  7% 45/625 [06:40<1:30:16,  9.34s/it]10/14/2024 23:35:40 - INFO - llamafactory.train.callbacks - {'loss': 1.0826, 'learning_rate': 1.9745e-04, 'epoch': 0.07, 'throughput': 861.12}\n",
            "{'loss': 1.0826, 'grad_norm': 0.5347787737846375, 'learning_rate': 0.00019745268727865774, 'epoch': 0.07, 'num_input_tokens_seen': 344480}\n",
            "  8% 50/625 [07:25<1:28:21,  9.22s/it]10/14/2024 23:36:26 - INFO - llamafactory.train.callbacks - {'loss': 1.1037, 'learning_rate': 1.9686e-04, 'epoch': 0.08, 'throughput': 860.86}\n",
            "{'loss': 1.1037, 'grad_norm': 0.7796570062637329, 'learning_rate': 0.0001968583161128631, 'epoch': 0.08, 'num_input_tokens_seen': 383872}\n",
            "  9% 55/625 [08:11<1:26:53,  9.15s/it]10/14/2024 23:37:12 - INFO - llamafactory.train.callbacks - {'loss': 1.0905, 'learning_rate': 1.9620e-04, 'epoch': 0.09, 'throughput': 860.32}\n",
            "{'loss': 1.0905, 'grad_norm': 0.5550365447998047, 'learning_rate': 0.0001962027671586086, 'epoch': 0.09, 'num_input_tokens_seen': 422608}\n",
            " 10% 60/625 [08:55<1:23:44,  8.89s/it]10/14/2024 23:37:56 - INFO - llamafactory.train.callbacks - {'loss': 1.0745, 'learning_rate': 1.9549e-04, 'epoch': 0.10, 'throughput': 860.26}\n",
            "{'loss': 1.0745, 'grad_norm': 0.6864750385284424, 'learning_rate': 0.00019548645447466431, 'epoch': 0.1, 'num_input_tokens_seen': 460992}\n",
            " 10% 65/625 [09:40<1:22:26,  8.83s/it]10/14/2024 23:38:41 - INFO - llamafactory.train.callbacks - {'loss': 1.1148, 'learning_rate': 1.9471e-04, 'epoch': 0.10, 'throughput': 859.95}\n",
            "{'loss': 1.1148, 'grad_norm': 0.7635449767112732, 'learning_rate': 0.00019470983049947444, 'epoch': 0.1, 'num_input_tokens_seen': 499056}\n",
            " 11% 70/625 [10:24<1:18:23,  8.48s/it]10/14/2024 23:39:25 - INFO - llamafactory.train.callbacks - {'loss': 1.2238, 'learning_rate': 1.9387e-04, 'epoch': 0.11, 'throughput': 859.86}\n",
            "{'loss': 1.2238, 'grad_norm': 1.5379966497421265, 'learning_rate': 0.00019387338576538744, 'epoch': 0.11, 'num_input_tokens_seen': 536976}\n",
            " 12% 75/625 [11:09<1:23:02,  9.06s/it]10/14/2024 23:40:10 - INFO - llamafactory.train.callbacks - {'loss': 1.0792, 'learning_rate': 1.9298e-04, 'epoch': 0.12, 'throughput': 859.65}\n",
            "{'loss': 1.0792, 'grad_norm': 0.5265907645225525, 'learning_rate': 0.00019297764858882514, 'epoch': 0.12, 'num_input_tokens_seen': 575248}\n",
            " 13% 80/625 [11:54<1:21:34,  8.98s/it]10/14/2024 23:40:55 - INFO - llamafactory.train.callbacks - {'loss': 1.0748, 'learning_rate': 1.9202e-04, 'epoch': 0.13, 'throughput': 859.45}\n",
            "{'loss': 1.0748, 'grad_norm': 0.5791109204292297, 'learning_rate': 0.00019202318473658705, 'epoch': 0.13, 'num_input_tokens_seen': 614272}\n",
            " 14% 85/625 [12:41<1:22:37,  9.18s/it]10/14/2024 23:41:42 - INFO - llamafactory.train.callbacks - {'loss': 1.1445, 'learning_rate': 1.9101e-04, 'epoch': 0.14, 'throughput': 859.21}\n",
            "{'loss': 1.1445, 'grad_norm': 0.6780444383621216, 'learning_rate': 0.00019101059706849957, 'epoch': 0.14, 'num_input_tokens_seen': 653936}\n",
            " 14% 90/625 [13:27<1:23:07,  9.32s/it]10/14/2024 23:42:28 - INFO - llamafactory.train.callbacks - {'loss': 1.1511, 'learning_rate': 1.8994e-04, 'epoch': 0.14, 'throughput': 858.71}\n",
            "{'loss': 1.1511, 'grad_norm': 0.5207211971282959, 'learning_rate': 0.0001899405251566371, 'epoch': 0.14, 'num_input_tokens_seen': 693600}\n",
            " 15% 95/625 [14:11<1:17:50,  8.81s/it]10/14/2024 23:43:12 - INFO - llamafactory.train.callbacks - {'loss': 1.0618, 'learning_rate': 1.8881e-04, 'epoch': 0.15, 'throughput': 858.12}\n",
            "{'loss': 1.0618, 'grad_norm': 0.5519402623176575, 'learning_rate': 0.00018881364488135448, 'epoch': 0.15, 'num_input_tokens_seen': 730432}\n",
            " 16% 100/625 [14:56<1:20:04,  9.15s/it]10/14/2024 23:43:57 - INFO - llamafactory.train.callbacks - {'loss': 1.0972, 'learning_rate': 1.8763e-04, 'epoch': 0.16, 'throughput': 857.64}\n",
            "{'loss': 1.0972, 'grad_norm': 0.5521262884140015, 'learning_rate': 0.00018763066800438636, 'epoch': 0.16, 'num_input_tokens_seen': 769088}\n",
            " 16% 100/625 [14:56<1:20:04,  9.15s/it][INFO|trainer.py:3503] 2024-10-14 23:43:57,676 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-100\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 23:43:58,183 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 23:43:58,184 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-14 23:43:58,302 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-14 23:43:58,302 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-100/special_tokens_map.json\n",
            " 17% 105/625 [15:43<1:19:45,  9.20s/it]10/14/2024 23:44:44 - INFO - llamafactory.train.callbacks - {'loss': 1.0393, 'learning_rate': 1.8639e-04, 'epoch': 0.17, 'throughput': 856.14}\n",
            "{'loss': 1.0393, 'grad_norm': 0.7044788599014282, 'learning_rate': 0.00018639234171928353, 'epoch': 0.17, 'num_input_tokens_seen': 808160}\n",
            " 18% 110/625 [16:30<1:20:12,  9.34s/it]10/14/2024 23:45:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0930, 'learning_rate': 1.8510e-04, 'epoch': 0.18, 'throughput': 856.01}\n",
            "{'loss': 1.093, 'grad_norm': 0.6210415959358215, 'learning_rate': 0.00018509944817946922, 'epoch': 0.18, 'num_input_tokens_seen': 848048}\n",
            " 18% 115/625 [17:15<1:15:39,  8.90s/it]10/14/2024 23:46:16 - INFO - llamafactory.train.callbacks - {'loss': 1.1086, 'learning_rate': 1.8375e-04, 'epoch': 0.18, 'throughput': 855.78}\n",
            "{'loss': 1.1086, 'grad_norm': 0.5671275854110718, 'learning_rate': 0.0001837528040042142, 'epoch': 0.18, 'num_input_tokens_seen': 886016}\n",
            " 19% 120/625 [18:00<1:16:03,  9.04s/it]10/14/2024 23:47:01 - INFO - llamafactory.train.callbacks - {'loss': 1.0647, 'learning_rate': 1.8235e-04, 'epoch': 0.19, 'throughput': 855.85}\n",
            "{'loss': 1.0647, 'grad_norm': 0.6013429164886475, 'learning_rate': 0.00018235325976284275, 'epoch': 0.19, 'num_input_tokens_seen': 924640}\n",
            " 20% 125/625 [18:44<1:14:53,  8.99s/it]10/14/2024 23:47:45 - INFO - llamafactory.train.callbacks - {'loss': 1.1211, 'learning_rate': 1.8090e-04, 'epoch': 0.20, 'throughput': 855.89}\n",
            "{'loss': 1.1211, 'grad_norm': 0.6283249855041504, 'learning_rate': 0.00018090169943749476, 'epoch': 0.2, 'num_input_tokens_seen': 962192}\n",
            " 21% 130/625 [19:29<1:15:23,  9.14s/it]10/14/2024 23:48:30 - INFO - llamafactory.train.callbacks - {'loss': 1.1189, 'learning_rate': 1.7940e-04, 'epoch': 0.21, 'throughput': 855.74}\n",
            "{'loss': 1.1189, 'grad_norm': 0.5306046605110168, 'learning_rate': 0.00017939903986478355, 'epoch': 0.21, 'num_input_tokens_seen': 1000576}\n",
            " 22% 135/625 [20:13<1:12:31,  8.88s/it]10/14/2024 23:49:14 - INFO - llamafactory.train.callbacks - {'loss': 0.9760, 'learning_rate': 1.7785e-04, 'epoch': 0.22, 'throughput': 855.65}\n",
            "{'loss': 0.976, 'grad_norm': 0.539962649345398, 'learning_rate': 0.00017784623015670238, 'epoch': 0.22, 'num_input_tokens_seen': 1038112}\n",
            " 22% 140/625 [20:57<1:11:43,  8.87s/it]10/14/2024 23:49:58 - INFO - llamafactory.train.callbacks - {'loss': 1.0906, 'learning_rate': 1.7624e-04, 'epoch': 0.22, 'throughput': 855.78}\n",
            "{'loss': 1.0906, 'grad_norm': 0.5589792728424072, 'learning_rate': 0.0001762442511011448, 'epoch': 0.22, 'num_input_tokens_seen': 1076336}\n",
            " 23% 145/625 [21:42<1:11:32,  8.94s/it]10/14/2024 23:50:43 - INFO - llamafactory.train.callbacks - {'loss': 1.0201, 'learning_rate': 1.7459e-04, 'epoch': 0.23, 'throughput': 855.68}\n",
            "{'loss': 1.0201, 'grad_norm': 0.7208147644996643, 'learning_rate': 0.00017459411454241822, 'epoch': 0.23, 'num_input_tokens_seen': 1114560}\n",
            " 24% 150/625 [22:28<1:12:43,  9.19s/it]10/14/2024 23:51:29 - INFO - llamafactory.train.callbacks - {'loss': 1.1243, 'learning_rate': 1.7290e-04, 'epoch': 0.24, 'throughput': 855.62}\n",
            "{'loss': 1.1243, 'grad_norm': 0.618073582649231, 'learning_rate': 0.00017289686274214118, 'epoch': 0.24, 'num_input_tokens_seen': 1154064}\n",
            " 25% 155/625 [23:12<1:09:46,  8.91s/it]10/14/2024 23:52:13 - INFO - llamafactory.train.callbacks - {'loss': 1.0858, 'learning_rate': 1.7115e-04, 'epoch': 0.25, 'throughput': 855.44}\n",
            "{'loss': 1.0858, 'grad_norm': 0.5705242156982422, 'learning_rate': 0.00017115356772092857, 'epoch': 0.25, 'num_input_tokens_seen': 1191408}\n",
            " 26% 160/625 [23:59<1:11:50,  9.27s/it]10/14/2024 23:53:00 - INFO - llamafactory.train.callbacks - {'loss': 1.2019, 'learning_rate': 1.6937e-04, 'epoch': 0.26, 'throughput': 855.47}\n",
            "{'loss': 1.2019, 'grad_norm': 0.63177889585495, 'learning_rate': 0.0001693653305812805, 'epoch': 0.26, 'num_input_tokens_seen': 1231296}\n",
            " 26% 165/625 [24:44<1:10:14,  9.16s/it]10/14/2024 23:53:45 - INFO - llamafactory.train.callbacks - {'loss': 1.0680, 'learning_rate': 1.6753e-04, 'epoch': 0.26, 'throughput': 855.47}\n",
            "{'loss': 1.068, 'grad_norm': 0.592242956161499, 'learning_rate': 0.00016753328081210245, 'epoch': 0.26, 'num_input_tokens_seen': 1270288}\n",
            " 27% 170/625 [25:31<1:09:13,  9.13s/it]10/14/2024 23:54:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0315, 'learning_rate': 1.6566e-04, 'epoch': 0.27, 'throughput': 855.47}\n",
            "{'loss': 1.0315, 'grad_norm': 0.6144363284111023, 'learning_rate': 0.00016565857557529566, 'epoch': 0.27, 'num_input_tokens_seen': 1309728}\n",
            " 28% 175/625 [26:15<1:06:39,  8.89s/it]10/14/2024 23:55:16 - INFO - llamafactory.train.callbacks - {'loss': 1.0309, 'learning_rate': 1.6374e-04, 'epoch': 0.28, 'throughput': 855.49}\n",
            "{'loss': 1.0309, 'grad_norm': 0.6009471416473389, 'learning_rate': 0.000163742398974869, 'epoch': 0.28, 'num_input_tokens_seen': 1347568}\n",
            " 29% 180/625 [27:00<1:07:36,  9.12s/it]10/14/2024 23:56:01 - INFO - llamafactory.train.callbacks - {'loss': 1.0534, 'learning_rate': 1.6179e-04, 'epoch': 0.29, 'throughput': 855.48}\n",
            "{'loss': 1.0534, 'grad_norm': 0.5439460873603821, 'learning_rate': 0.00016178596130903344, 'epoch': 0.29, 'num_input_tokens_seen': 1386720}\n",
            " 30% 185/625 [27:47<1:07:36,  9.22s/it]10/14/2024 23:56:48 - INFO - llamafactory.train.callbacks - {'loss': 1.0358, 'learning_rate': 1.5979e-04, 'epoch': 0.30, 'throughput': 855.55}\n",
            "{'loss': 1.0358, 'grad_norm': 0.5166096687316895, 'learning_rate': 0.0001597904983057519, 'epoch': 0.3, 'num_input_tokens_seen': 1426320}\n",
            " 30% 190/625 [28:31<1:04:10,  8.85s/it]10/14/2024 23:57:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0190, 'learning_rate': 1.5776e-04, 'epoch': 0.30, 'throughput': 855.4}\n",
            "{'loss': 1.019, 'grad_norm': 0.5638114809989929, 'learning_rate': 0.00015775727034222675, 'epoch': 0.3, 'num_input_tokens_seen': 1463616}\n",
            " 31% 195/625 [29:14<1:03:27,  8.85s/it]10/14/2024 23:58:15 - INFO - llamafactory.train.callbacks - {'loss': 1.0785, 'learning_rate': 1.5569e-04, 'epoch': 0.31, 'throughput': 855.32}\n",
            "{'loss': 1.0785, 'grad_norm': 0.6753865480422974, 'learning_rate': 0.00015568756164881882, 'epoch': 0.31, 'num_input_tokens_seen': 1500688}\n",
            " 32% 200/625 [29:59<1:04:43,  9.14s/it]10/14/2024 23:59:00 - INFO - llamafactory.train.callbacks - {'loss': 1.1521, 'learning_rate': 1.5358e-04, 'epoch': 0.32, 'throughput': 855.28}\n",
            "{'loss': 1.1521, 'grad_norm': 0.5848162770271301, 'learning_rate': 0.00015358267949789966, 'epoch': 0.32, 'num_input_tokens_seen': 1539424}\n",
            " 32% 200/625 [29:59<1:04:43,  9.14s/it][INFO|trainer.py:3503] 2024-10-14 23:59:00,834 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-200\n",
            "[INFO|configuration_utils.py:733] 2024-10-14 23:59:01,372 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-14 23:59:01,373 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-14 23:59:01,503 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-14 23:59:01,503 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-200/special_tokens_map.json\n",
            " 33% 205/625 [30:46<1:04:21,  9.19s/it]10/14/2024 23:59:47 - INFO - llamafactory.train.callbacks - {'loss': 1.0911, 'learning_rate': 1.5144e-04, 'epoch': 0.33, 'throughput': 854.62}\n",
            "{'loss': 1.0911, 'grad_norm': 0.55477374792099, 'learning_rate': 0.00015144395337815064, 'epoch': 0.33, 'num_input_tokens_seen': 1578176}\n",
            " 34% 210/625 [31:32<1:04:24,  9.31s/it]10/15/2024 00:00:33 - INFO - llamafactory.train.callbacks - {'loss': 1.0513, 'learning_rate': 1.4927e-04, 'epoch': 0.34, 'throughput': 854.41}\n",
            "{'loss': 1.0513, 'grad_norm': 0.51356041431427, 'learning_rate': 0.00014927273415482915, 'epoch': 0.34, 'num_input_tokens_seen': 1617232}\n",
            " 34% 215/625 [32:19<1:01:41,  9.03s/it]10/15/2024 00:01:19 - INFO - llamafactory.train.callbacks - {'loss': 1.1312, 'learning_rate': 1.4707e-04, 'epoch': 0.34, 'throughput': 854.22}\n",
            "{'loss': 1.1312, 'grad_norm': 1.1056444644927979, 'learning_rate': 0.0001470703932165333, 'epoch': 0.34, 'num_input_tokens_seen': 1656352}\n",
            " 35% 220/625 [33:03<1:00:02,  8.90s/it]10/15/2024 00:02:04 - INFO - llamafactory.train.callbacks - {'loss': 1.1441, 'learning_rate': 1.4484e-04, 'epoch': 0.35, 'throughput': 854.41}\n",
            "{'loss': 1.1441, 'grad_norm': 0.6645542979240417, 'learning_rate': 0.00014483832160900326, 'epoch': 0.35, 'num_input_tokens_seen': 1694688}\n",
            " 36% 225/625 [33:50<1:02:12,  9.33s/it]10/15/2024 00:02:51 - INFO - llamafactory.train.callbacks - {'loss': 1.0453, 'learning_rate': 1.4258e-04, 'epoch': 0.36, 'throughput': 854.5}\n",
            "{'loss': 1.0453, 'grad_norm': 0.6346023678779602, 'learning_rate': 0.00014257792915650728, 'epoch': 0.36, 'num_input_tokens_seen': 1734784}\n",
            " 37% 230/625 [34:34<59:52,  9.10s/it]10/15/2024 00:03:35 - INFO - llamafactory.train.callbacks - {'loss': 1.1449, 'learning_rate': 1.4029e-04, 'epoch': 0.37, 'throughput': 854.61}\n",
            "{'loss': 1.1449, 'grad_norm': 0.6903899908065796, 'learning_rate': 0.00014029064357136628, 'epoch': 0.37, 'num_input_tokens_seen': 1772720}\n",
            " 38% 235/625 [35:20<58:54,  9.06s/it]  10/15/2024 00:04:21 - INFO - llamafactory.train.callbacks - {'loss': 1.0707, 'learning_rate': 1.3798e-04, 'epoch': 0.38, 'throughput': 854.54}\n",
            "{'loss': 1.0707, 'grad_norm': 0.7894919514656067, 'learning_rate': 0.00013797790955218014, 'epoch': 0.38, 'num_input_tokens_seen': 1811792}\n",
            " 38% 240/625 [36:03<56:54,  8.87s/it]10/15/2024 00:05:04 - INFO - llamafactory.train.callbacks - {'loss': 1.2106, 'learning_rate': 1.3564e-04, 'epoch': 0.38, 'throughput': 854.59}\n",
            "{'loss': 1.2106, 'grad_norm': 0.5539196729660034, 'learning_rate': 0.00013564118787132506, 'epoch': 0.38, 'num_input_tokens_seen': 1849296}\n",
            " 39% 245/625 [36:50<58:11,  9.19s/it]10/15/2024 00:05:51 - INFO - llamafactory.train.callbacks - {'loss': 1.0326, 'learning_rate': 1.3328e-04, 'epoch': 0.39, 'throughput': 854.64}\n",
            "{'loss': 1.0326, 'grad_norm': 0.5574841499328613, 'learning_rate': 0.00013328195445229868, 'epoch': 0.39, 'num_input_tokens_seen': 1889104}\n",
            " 40% 250/625 [37:35<55:30,  8.88s/it]10/15/2024 00:06:36 - INFO - llamafactory.train.callbacks - {'loss': 0.9123, 'learning_rate': 1.3090e-04, 'epoch': 0.40, 'throughput': 854.55}\n",
            "{'loss': 0.9123, 'grad_norm': 0.6699144244194031, 'learning_rate': 0.00013090169943749476, 'epoch': 0.4, 'num_input_tokens_seen': 1927440}\n",
            " 41% 255/625 [38:18<54:13,  8.79s/it]10/15/2024 00:07:19 - INFO - llamafactory.train.callbacks - {'loss': 1.0743, 'learning_rate': 1.2850e-04, 'epoch': 0.41, 'throughput': 854.37}\n",
            "{'loss': 1.0743, 'grad_norm': 0.5838106870651245, 'learning_rate': 0.0001285019262469976, 'epoch': 0.41, 'num_input_tokens_seen': 1964192}\n",
            " 42% 260/625 [39:04<55:08,  9.06s/it]10/15/2024 00:08:05 - INFO - llamafactory.train.callbacks - {'loss': 1.0266, 'learning_rate': 1.2608e-04, 'epoch': 0.42, 'throughput': 854.36}\n",
            "{'loss': 1.0266, 'grad_norm': 0.5316107273101807, 'learning_rate': 0.00012608415062898972, 'epoch': 0.42, 'num_input_tokens_seen': 2003120}\n",
            " 42% 265/625 [39:47<52:17,  8.71s/it]10/15/2024 00:08:48 - INFO - llamafactory.train.callbacks - {'loss': 1.0370, 'learning_rate': 1.2365e-04, 'epoch': 0.42, 'throughput': 854.19}\n",
            "{'loss': 1.037, 'grad_norm': 0.6364112496376038, 'learning_rate': 0.00012364989970237248, 'epoch': 0.42, 'num_input_tokens_seen': 2039616}\n",
            " 43% 270/625 [40:34<54:40,  9.24s/it]10/15/2024 00:09:35 - INFO - llamafactory.train.callbacks - {'loss': 1.0976, 'learning_rate': 1.2120e-04, 'epoch': 0.43, 'throughput': 854.19}\n",
            "{'loss': 1.0976, 'grad_norm': 0.5001750588417053, 'learning_rate': 0.00012120071099220549, 'epoch': 0.43, 'num_input_tokens_seen': 2079216}\n",
            " 44% 275/625 [41:16<49:40,  8.52s/it]10/15/2024 00:10:17 - INFO - llamafactory.train.callbacks - {'loss': 1.0085, 'learning_rate': 1.1874e-04, 'epoch': 0.44, 'throughput': 854.1}\n",
            "{'loss': 1.0085, 'grad_norm': 0.9062410593032837, 'learning_rate': 0.00011873813145857249, 'epoch': 0.44, 'num_input_tokens_seen': 2115520}\n",
            " 45% 280/625 [42:01<50:29,  8.78s/it]10/15/2024 00:11:02 - INFO - llamafactory.train.callbacks - {'loss': 1.0912, 'learning_rate': 1.1626e-04, 'epoch': 0.45, 'throughput': 854.16}\n",
            "{'loss': 1.0912, 'grad_norm': 0.7243922352790833, 'learning_rate': 0.00011626371651948838, 'epoch': 0.45, 'num_input_tokens_seen': 2153760}\n",
            " 46% 285/625 [42:47<51:13,  9.04s/it]10/15/2024 00:11:48 - INFO - llamafactory.train.callbacks - {'loss': 1.1221, 'learning_rate': 1.1378e-04, 'epoch': 0.46, 'throughput': 854.12}\n",
            "{'loss': 1.1221, 'grad_norm': 0.6941623687744141, 'learning_rate': 0.0001137790290684638, 'epoch': 0.46, 'num_input_tokens_seen': 2192672}\n",
            " 46% 290/625 [43:31<50:19,  9.01s/it]10/15/2024 00:12:32 - INFO - llamafactory.train.callbacks - {'loss': 1.1360, 'learning_rate': 1.1129e-04, 'epoch': 0.46, 'throughput': 854.08}\n",
            "{'loss': 1.136, 'grad_norm': 0.518869936466217, 'learning_rate': 0.00011128563848734816, 'epoch': 0.46, 'num_input_tokens_seen': 2230624}\n",
            " 47% 295/625 [44:14<47:36,  8.66s/it]10/15/2024 00:13:15 - INFO - llamafactory.train.callbacks - {'loss': 1.0470, 'learning_rate': 1.0879e-04, 'epoch': 0.47, 'throughput': 854.04}\n",
            "{'loss': 1.047, 'grad_norm': 0.5539364814758301, 'learning_rate': 0.00010878511965507434, 'epoch': 0.47, 'num_input_tokens_seen': 2267280}\n",
            " 48% 300/625 [44:58<47:55,  8.85s/it]10/15/2024 00:13:59 - INFO - llamafactory.train.callbacks - {'loss': 1.1782, 'learning_rate': 1.0628e-04, 'epoch': 0.48, 'throughput': 854.07}\n",
            "{'loss': 1.1782, 'grad_norm': 0.5539495348930359, 'learning_rate': 0.00010627905195293135, 'epoch': 0.48, 'num_input_tokens_seen': 2304368}\n",
            " 48% 300/625 [44:58<47:55,  8.85s/it][INFO|trainer.py:3503] 2024-10-15 00:13:59,033 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-300\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 00:13:59,558 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 00:13:59,559 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 00:13:59,721 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 00:13:59,721 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-300/special_tokens_map.json\n",
            " 49% 305/625 [45:46<50:13,  9.42s/it]10/15/2024 00:14:47 - INFO - llamafactory.train.callbacks - {'loss': 1.1020, 'learning_rate': 1.0377e-04, 'epoch': 0.49, 'throughput': 853.54}\n",
            "{'loss': 1.102, 'grad_norm': 0.5847554206848145, 'learning_rate': 0.00010376901826699348, 'epoch': 0.49, 'num_input_tokens_seen': 2344592}\n",
            " 50% 310/625 [46:32<48:09,  9.17s/it]10/15/2024 00:15:33 - INFO - llamafactory.train.callbacks - {'loss': 1.0591, 'learning_rate': 1.0126e-04, 'epoch': 0.50, 'throughput': 853.55}\n",
            "{'loss': 1.0591, 'grad_norm': 0.5844855308532715, 'learning_rate': 0.00010125660398833528, 'epoch': 0.5, 'num_input_tokens_seen': 2383504}\n",
            " 50% 315/625 [47:19<47:37,  9.22s/it]10/15/2024 00:16:20 - INFO - llamafactory.train.callbacks - {'loss': 1.0606, 'learning_rate': 9.8743e-05, 'epoch': 0.50, 'throughput': 853.58}\n",
            "{'loss': 1.0606, 'grad_norm': 0.6370247006416321, 'learning_rate': 9.874339601166473e-05, 'epoch': 0.5, 'num_input_tokens_seen': 2423520}\n",
            " 51% 320/625 [48:03<46:09,  9.08s/it]10/15/2024 00:17:04 - INFO - llamafactory.train.callbacks - {'loss': 1.1102, 'learning_rate': 9.6231e-05, 'epoch': 0.51, 'throughput': 853.56}\n",
            "{'loss': 1.1102, 'grad_norm': 0.49232780933380127, 'learning_rate': 9.623098173300654e-05, 'epoch': 0.51, 'num_input_tokens_seen': 2461408}\n",
            " 52% 325/625 [48:48<44:49,  8.96s/it]10/15/2024 00:17:49 - INFO - llamafactory.train.callbacks - {'loss': 1.1673, 'learning_rate': 9.3721e-05, 'epoch': 0.52, 'throughput': 853.45}\n",
            "{'loss': 1.1673, 'grad_norm': 0.5953924655914307, 'learning_rate': 9.372094804706867e-05, 'epoch': 0.52, 'num_input_tokens_seen': 2499712}\n",
            " 53% 330/625 [49:31<42:22,  8.62s/it]10/15/2024 00:18:32 - INFO - llamafactory.train.callbacks - {'loss': 1.0611, 'learning_rate': 9.1215e-05, 'epoch': 0.53, 'throughput': 853.44}\n",
            "{'loss': 1.0611, 'grad_norm': 0.5736607313156128, 'learning_rate': 9.121488034492569e-05, 'epoch': 0.53, 'num_input_tokens_seen': 2535968}\n",
            " 54% 335/625 [50:15<42:11,  8.73s/it]10/15/2024 00:19:16 - INFO - llamafactory.train.callbacks - {'loss': 1.1236, 'learning_rate': 8.8714e-05, 'epoch': 0.54, 'throughput': 853.4}\n",
            "{'loss': 1.1236, 'grad_norm': 0.7327518463134766, 'learning_rate': 8.871436151265184e-05, 'epoch': 0.54, 'num_input_tokens_seen': 2573520}\n",
            " 54% 340/625 [50:59<41:20,  8.70s/it]10/15/2024 00:20:00 - INFO - llamafactory.train.callbacks - {'loss': 1.0754, 'learning_rate': 8.6221e-05, 'epoch': 0.54, 'throughput': 853.43}\n",
            "{'loss': 1.0754, 'grad_norm': 0.6164984107017517, 'learning_rate': 8.62209709315362e-05, 'epoch': 0.54, 'num_input_tokens_seen': 2611056}\n",
            " 55% 345/625 [51:45<41:57,  8.99s/it]10/15/2024 00:20:46 - INFO - llamafactory.train.callbacks - {'loss': 1.0970, 'learning_rate': 8.3736e-05, 'epoch': 0.55, 'throughput': 853.5}\n",
            "{'loss': 1.097, 'grad_norm': 0.5900065898895264, 'learning_rate': 8.373628348051165e-05, 'epoch': 0.55, 'num_input_tokens_seen': 2650464}\n",
            " 56% 350/625 [52:30<41:13,  9.00s/it]10/15/2024 00:21:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0398, 'learning_rate': 8.1262e-05, 'epoch': 0.56, 'throughput': 853.45}\n",
            "{'loss': 1.0398, 'grad_norm': 0.6398114562034607, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.56, 'num_input_tokens_seen': 2688944}\n",
            " 57% 355/625 [53:15<39:57,  8.88s/it]10/15/2024 00:22:16 - INFO - llamafactory.train.callbacks - {'loss': 1.0862, 'learning_rate': 7.8799e-05, 'epoch': 0.57, 'throughput': 853.42}\n",
            "{'loss': 1.0862, 'grad_norm': 0.7125230431556702, 'learning_rate': 7.879928900779456e-05, 'epoch': 0.57, 'num_input_tokens_seen': 2727344}\n",
            " 58% 360/625 [54:02<41:00,  9.28s/it]10/15/2024 00:23:03 - INFO - llamafactory.train.callbacks - {'loss': 1.0656, 'learning_rate': 7.6350e-05, 'epoch': 0.58, 'throughput': 853.44}\n",
            "{'loss': 1.0656, 'grad_norm': 0.5792780518531799, 'learning_rate': 7.635010029762756e-05, 'epoch': 0.58, 'num_input_tokens_seen': 2767360}\n",
            " 58% 365/625 [54:47<39:39,  9.15s/it]10/15/2024 00:23:48 - INFO - llamafactory.train.callbacks - {'loss': 0.9965, 'learning_rate': 7.3916e-05, 'epoch': 0.58, 'throughput': 853.42}\n",
            "{'loss': 0.9965, 'grad_norm': 0.5198700428009033, 'learning_rate': 7.391584937101033e-05, 'epoch': 0.58, 'num_input_tokens_seen': 2805680}\n",
            " 59% 370/625 [55:32<38:28,  9.05s/it]10/15/2024 00:24:33 - INFO - llamafactory.train.callbacks - {'loss': 0.9316, 'learning_rate': 7.1498e-05, 'epoch': 0.59, 'throughput': 853.45}\n",
            "{'loss': 0.9316, 'grad_norm': 0.6670189499855042, 'learning_rate': 7.149807375300239e-05, 'epoch': 0.59, 'num_input_tokens_seen': 2844464}\n",
            " 60% 375/625 [56:18<37:26,  8.99s/it]10/15/2024 00:25:18 - INFO - llamafactory.train.callbacks - {'loss': 1.1284, 'learning_rate': 6.9098e-05, 'epoch': 0.60, 'throughput': 853.5}\n",
            "{'loss': 1.1284, 'grad_norm': 0.6528822779655457, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.6, 'num_input_tokens_seen': 2883136}\n",
            " 61% 380/625 [57:02<36:25,  8.92s/it]10/15/2024 00:26:03 - INFO - llamafactory.train.callbacks - {'loss': 1.0168, 'learning_rate': 6.6718e-05, 'epoch': 0.61, 'throughput': 853.59}\n",
            "{'loss': 1.0168, 'grad_norm': 0.5695247650146484, 'learning_rate': 6.671804554770135e-05, 'epoch': 0.61, 'num_input_tokens_seen': 2921440}\n",
            " 62% 385/625 [57:46<35:44,  8.94s/it]10/15/2024 00:26:47 - INFO - llamafactory.train.callbacks - {'loss': 1.1005, 'learning_rate': 6.4359e-05, 'epoch': 0.62, 'throughput': 853.59}\n",
            "{'loss': 1.1005, 'grad_norm': 0.7716062664985657, 'learning_rate': 6.435881212867493e-05, 'epoch': 0.62, 'num_input_tokens_seen': 2959296}\n",
            " 62% 390/625 [58:32<35:36,  9.09s/it]10/15/2024 00:27:33 - INFO - llamafactory.train.callbacks - {'loss': 1.0503, 'learning_rate': 6.2022e-05, 'epoch': 0.62, 'throughput': 853.56}\n",
            "{'loss': 1.0503, 'grad_norm': 0.5111759305000305, 'learning_rate': 6.20220904478199e-05, 'epoch': 0.62, 'num_input_tokens_seen': 2998096}\n",
            " 63% 395/625 [59:16<34:12,  8.92s/it]10/15/2024 00:28:17 - INFO - llamafactory.train.callbacks - {'loss': 1.2086, 'learning_rate': 5.9709e-05, 'epoch': 0.63, 'throughput': 853.55}\n",
            "{'loss': 1.2086, 'grad_norm': 0.6094025373458862, 'learning_rate': 5.9709356428633746e-05, 'epoch': 0.63, 'num_input_tokens_seen': 3035504}\n",
            " 64% 400/625 [1:00:00<33:38,  8.97s/it]10/15/2024 00:29:01 - INFO - llamafactory.train.callbacks - {'loss': 1.1211, 'learning_rate': 5.7422e-05, 'epoch': 0.64, 'throughput': 853.56}\n",
            "{'loss': 1.1211, 'grad_norm': 0.6491724848747253, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.64, 'num_input_tokens_seen': 3073568}\n",
            " 64% 400/625 [1:00:00<33:38,  8.97s/it][INFO|trainer.py:3503] 2024-10-15 00:29:01,794 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-400\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 00:29:02,310 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 00:29:02,312 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 00:29:02,499 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 00:29:02,499 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-400/special_tokens_map.json\n",
            " 65% 405/625 [1:00:46<33:15,  9.07s/it]10/15/2024 00:29:47 - INFO - llamafactory.train.callbacks - {'loss': 1.1478, 'learning_rate': 5.5162e-05, 'epoch': 0.65, 'throughput': 853.16}\n",
            "{'loss': 1.1478, 'grad_norm': 0.7043200731277466, 'learning_rate': 5.5161678390996796e-05, 'epoch': 0.65, 'num_input_tokens_seen': 3111168}\n",
            " 66% 410/625 [1:01:31<31:51,  8.89s/it]10/15/2024 00:30:32 - INFO - llamafactory.train.callbacks - {'loss': 1.0070, 'learning_rate': 5.2930e-05, 'epoch': 0.66, 'throughput': 853.17}\n",
            "{'loss': 1.007, 'grad_norm': 0.5435596108436584, 'learning_rate': 5.292960678346675e-05, 'epoch': 0.66, 'num_input_tokens_seen': 3149504}\n",
            " 66% 415/625 [1:02:14<29:59,  8.57s/it]10/15/2024 00:31:15 - INFO - llamafactory.train.callbacks - {'loss': 1.1351, 'learning_rate': 5.0727e-05, 'epoch': 0.66, 'throughput': 853.22}\n",
            "{'loss': 1.1351, 'grad_norm': 0.7268032431602478, 'learning_rate': 5.072726584517086e-05, 'epoch': 0.66, 'num_input_tokens_seen': 3186592}\n",
            " 67% 420/625 [1:02:59<30:14,  8.85s/it]10/15/2024 00:32:00 - INFO - llamafactory.train.callbacks - {'loss': 1.0926, 'learning_rate': 4.8556e-05, 'epoch': 0.67, 'throughput': 853.23}\n",
            "{'loss': 1.0926, 'grad_norm': 0.5702106356620789, 'learning_rate': 4.8556046621849346e-05, 'epoch': 0.67, 'num_input_tokens_seen': 3224912}\n",
            " 68% 425/625 [1:03:44<30:11,  9.06s/it]10/15/2024 00:32:45 - INFO - llamafactory.train.callbacks - {'loss': 1.0219, 'learning_rate': 4.6417e-05, 'epoch': 0.68, 'throughput': 853.27}\n",
            "{'loss': 1.0219, 'grad_norm': 0.5588555932044983, 'learning_rate': 4.6417320502100316e-05, 'epoch': 0.68, 'num_input_tokens_seen': 3263760}\n",
            " 69% 430/625 [1:04:30<29:26,  9.06s/it]10/15/2024 00:33:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0646, 'learning_rate': 4.4312e-05, 'epoch': 0.69, 'throughput': 853.29}\n",
            "{'loss': 1.0646, 'grad_norm': 0.6753600239753723, 'learning_rate': 4.431243835118124e-05, 'epoch': 0.69, 'num_input_tokens_seen': 3302688}\n",
            " 70% 435/625 [1:05:15<28:30,  9.01s/it]10/15/2024 00:34:16 - INFO - llamafactory.train.callbacks - {'loss': 1.1550, 'learning_rate': 4.2243e-05, 'epoch': 0.70, 'throughput': 853.37}\n",
            "{'loss': 1.155, 'grad_norm': 0.5588560104370117, 'learning_rate': 4.224272965777326e-05, 'epoch': 0.7, 'num_input_tokens_seen': 3341088}\n",
            " 70% 440/625 [1:06:01<28:25,  9.22s/it]10/15/2024 00:35:02 - INFO - llamafactory.train.callbacks - {'loss': 1.0159, 'learning_rate': 4.0210e-05, 'epoch': 0.70, 'throughput': 853.32}\n",
            "{'loss': 1.0159, 'grad_norm': 0.5534396171569824, 'learning_rate': 4.020950169424815e-05, 'epoch': 0.7, 'num_input_tokens_seen': 3380336}\n",
            " 71% 445/625 [1:06:46<27:27,  9.15s/it]10/15/2024 00:35:47 - INFO - llamafactory.train.callbacks - {'loss': 1.0634, 'learning_rate': 3.8214e-05, 'epoch': 0.71, 'throughput': 853.35}\n",
            "{'loss': 1.0634, 'grad_norm': 0.5367709994316101, 'learning_rate': 3.821403869096658e-05, 'epoch': 0.71, 'num_input_tokens_seen': 3419088}\n",
            " 72% 450/625 [1:07:30<25:49,  8.85s/it]10/15/2024 00:36:31 - INFO - llamafactory.train.callbacks - {'loss': 1.1168, 'learning_rate': 3.6258e-05, 'epoch': 0.72, 'throughput': 853.34}\n",
            "{'loss': 1.1168, 'grad_norm': 0.5950206518173218, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.72, 'num_input_tokens_seen': 3456272}\n",
            " 73% 455/625 [1:08:13<24:07,  8.51s/it]10/15/2024 00:37:14 - INFO - llamafactory.train.callbacks - {'loss': 0.9720, 'learning_rate': 3.4341e-05, 'epoch': 0.73, 'throughput': 853.37}\n",
            "{'loss': 0.972, 'grad_norm': 0.6492903828620911, 'learning_rate': 3.4341424424704375e-05, 'epoch': 0.73, 'num_input_tokens_seen': 3493168}\n",
            " 74% 460/625 [1:08:59<25:11,  9.16s/it]10/15/2024 00:38:00 - INFO - llamafactory.train.callbacks - {'loss': 1.1123, 'learning_rate': 3.2467e-05, 'epoch': 0.74, 'throughput': 853.35}\n",
            "{'loss': 1.1123, 'grad_norm': 0.5837770104408264, 'learning_rate': 3.246671918789755e-05, 'epoch': 0.74, 'num_input_tokens_seen': 3532384}\n",
            " 74% 465/625 [1:09:46<24:33,  9.21s/it]10/15/2024 00:38:46 - INFO - llamafactory.train.callbacks - {'loss': 1.0696, 'learning_rate': 3.0635e-05, 'epoch': 0.74, 'throughput': 853.37}\n",
            "{'loss': 1.0696, 'grad_norm': 0.6068534255027771, 'learning_rate': 3.063466941871952e-05, 'epoch': 0.74, 'num_input_tokens_seen': 3572272}\n",
            " 75% 470/625 [1:10:30<23:14,  9.00s/it]10/15/2024 00:39:31 - INFO - llamafactory.train.callbacks - {'loss': 1.0498, 'learning_rate': 2.8846e-05, 'epoch': 0.75, 'throughput': 853.43}\n",
            "{'loss': 1.0498, 'grad_norm': 0.5134088397026062, 'learning_rate': 2.8846432279071467e-05, 'epoch': 0.75, 'num_input_tokens_seen': 3610752}\n",
            " 76% 475/625 [1:11:17<22:55,  9.17s/it]10/15/2024 00:40:18 - INFO - llamafactory.train.callbacks - {'loss': 1.0697, 'learning_rate': 2.7103e-05, 'epoch': 0.76, 'throughput': 853.46}\n",
            "{'loss': 1.0697, 'grad_norm': 0.5388484597206116, 'learning_rate': 2.7103137257858868e-05, 'epoch': 0.76, 'num_input_tokens_seen': 3650464}\n",
            " 77% 480/625 [1:12:02<21:57,  9.08s/it]10/15/2024 00:41:03 - INFO - llamafactory.train.callbacks - {'loss': 1.0019, 'learning_rate': 2.5406e-05, 'epoch': 0.77, 'throughput': 853.48}\n",
            "{'loss': 1.0019, 'grad_norm': 0.5597943067550659, 'learning_rate': 2.540588545758179e-05, 'epoch': 0.77, 'num_input_tokens_seen': 3688976}\n",
            " 78% 485/625 [1:12:47<21:20,  9.15s/it]10/15/2024 00:41:48 - INFO - llamafactory.train.callbacks - {'loss': 1.0733, 'learning_rate': 2.3756e-05, 'epoch': 0.78, 'throughput': 853.42}\n",
            "{'loss': 1.0733, 'grad_norm': 0.6357629299163818, 'learning_rate': 2.37557488988552e-05, 'epoch': 0.78, 'num_input_tokens_seen': 3727472}\n",
            " 78% 490/625 [1:13:32<20:06,  8.94s/it]10/15/2024 00:42:33 - INFO - llamafactory.train.callbacks - {'loss': 1.0349, 'learning_rate': 2.2154e-05, 'epoch': 0.78, 'throughput': 853.48}\n",
            "{'loss': 1.0349, 'grad_norm': 0.5960575342178345, 'learning_rate': 2.2153769843297667e-05, 'epoch': 0.78, 'num_input_tokens_seen': 3766112}\n",
            " 79% 495/625 [1:14:17<19:05,  8.81s/it]10/15/2024 00:43:18 - INFO - llamafactory.train.callbacks - {'loss': 1.0475, 'learning_rate': 2.0601e-05, 'epoch': 0.79, 'throughput': 853.56}\n",
            "{'loss': 1.0475, 'grad_norm': 0.6362245082855225, 'learning_rate': 2.0600960135216462e-05, 'epoch': 0.79, 'num_input_tokens_seen': 3804400}\n",
            " 80% 500/625 [1:15:01<18:58,  9.11s/it]10/15/2024 00:44:02 - INFO - llamafactory.train.callbacks - {'loss': 1.1045, 'learning_rate': 1.9098e-05, 'epoch': 0.80, 'throughput': 853.59}\n",
            "{'loss': 1.1045, 'grad_norm': 0.5570360422134399, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.8, 'num_input_tokens_seen': 3842736}\n",
            " 80% 500/625 [1:15:01<18:58,  9.11s/it][INFO|trainer.py:3503] 2024-10-15 00:44:02,762 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-500\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 00:44:03,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 00:44:03,295 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 00:44:03,468 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 00:44:03,468 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-500/special_tokens_map.json\n",
            " 81% 505/625 [1:15:48<18:17,  9.15s/it]10/15/2024 00:44:49 - INFO - llamafactory.train.callbacks - {'loss': 1.0988, 'learning_rate': 1.7647e-05, 'epoch': 0.81, 'throughput': 853.25}\n",
            "{'loss': 1.0988, 'grad_norm': 1.4007748365402222, 'learning_rate': 1.7646740237157256e-05, 'epoch': 0.81, 'num_input_tokens_seen': 3881264}\n",
            " 82% 510/625 [1:16:35<17:49,  9.30s/it]10/15/2024 00:45:36 - INFO - llamafactory.train.callbacks - {'loss': 0.9936, 'learning_rate': 1.6247e-05, 'epoch': 0.82, 'throughput': 853.29}\n",
            "{'loss': 0.9936, 'grad_norm': 0.5073487758636475, 'learning_rate': 1.6247195995785837e-05, 'epoch': 0.82, 'num_input_tokens_seen': 3920944}\n",
            " 82% 515/625 [1:17:20<16:46,  9.15s/it]10/15/2024 00:46:20 - INFO - llamafactory.train.callbacks - {'loss': 0.9306, 'learning_rate': 1.4901e-05, 'epoch': 0.82, 'throughput': 853.32}\n",
            "{'loss': 0.9306, 'grad_norm': 0.5765300989151001, 'learning_rate': 1.4900551820530828e-05, 'epoch': 0.82, 'num_input_tokens_seen': 3959440}\n",
            " 83% 520/625 [1:18:04<15:19,  8.76s/it]10/15/2024 00:47:05 - INFO - llamafactory.train.callbacks - {'loss': 0.9983, 'learning_rate': 1.3608e-05, 'epoch': 0.83, 'throughput': 853.36}\n",
            "{'loss': 0.9983, 'grad_norm': 0.6526814699172974, 'learning_rate': 1.3607658280716473e-05, 'epoch': 0.83, 'num_input_tokens_seen': 3997440}\n",
            " 84% 525/625 [1:18:46<14:01,  8.41s/it]10/15/2024 00:47:47 - INFO - llamafactory.train.callbacks - {'loss': 1.1483, 'learning_rate': 1.2369e-05, 'epoch': 0.84, 'throughput': 853.41}\n",
            "{'loss': 1.1483, 'grad_norm': 0.7276056408882141, 'learning_rate': 1.2369331995613665e-05, 'epoch': 0.84, 'num_input_tokens_seen': 4033392}\n",
            " 85% 530/625 [1:19:31<14:20,  9.06s/it]10/15/2024 00:48:32 - INFO - llamafactory.train.callbacks - {'loss': 1.0063, 'learning_rate': 1.1186e-05, 'epoch': 0.85, 'throughput': 853.37}\n",
            "{'loss': 1.0063, 'grad_norm': 0.5690320134162903, 'learning_rate': 1.1186355118645554e-05, 'epoch': 0.85, 'num_input_tokens_seen': 4072224}\n",
            " 86% 535/625 [1:20:16<13:27,  8.98s/it]10/15/2024 00:49:16 - INFO - llamafactory.train.callbacks - {'loss': 1.0424, 'learning_rate': 1.0059e-05, 'epoch': 0.86, 'throughput': 853.37}\n",
            "{'loss': 1.0424, 'grad_norm': 0.5430582165718079, 'learning_rate': 1.0059474843362892e-05, 'epoch': 0.86, 'num_input_tokens_seen': 4109888}\n",
            " 86% 540/625 [1:21:01<12:44,  8.99s/it]10/15/2024 00:50:02 - INFO - llamafactory.train.callbacks - {'loss': 1.1063, 'learning_rate': 8.9894e-06, 'epoch': 0.86, 'throughput': 853.44}\n",
            "{'loss': 1.1063, 'grad_norm': 0.6301396489143372, 'learning_rate': 8.989402931500434e-06, 'epoch': 0.86, 'num_input_tokens_seen': 4148976}\n",
            " 87% 545/625 [1:21:47<12:00,  9.01s/it]10/15/2024 00:50:47 - INFO - llamafactory.train.callbacks - {'loss': 0.9815, 'learning_rate': 7.9768e-06, 'epoch': 0.87, 'throughput': 853.47}\n",
            "{'loss': 0.9815, 'grad_norm': 0.680840015411377, 'learning_rate': 7.976815263412963e-06, 'epoch': 0.87, 'num_input_tokens_seen': 4187984}\n",
            " 88% 550/625 [1:22:29<10:50,  8.67s/it]10/15/2024 00:51:30 - INFO - llamafactory.train.callbacks - {'loss': 1.0464, 'learning_rate': 7.0224e-06, 'epoch': 0.88, 'throughput': 853.48}\n",
            "{'loss': 1.0464, 'grad_norm': 0.5650590062141418, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.88, 'num_input_tokens_seen': 4224512}\n",
            " 89% 555/625 [1:23:14<10:25,  8.93s/it]10/15/2024 00:52:15 - INFO - llamafactory.train.callbacks - {'loss': 1.0404, 'learning_rate': 6.1266e-06, 'epoch': 0.89, 'throughput': 853.5}\n",
            "{'loss': 1.0404, 'grad_norm': 0.5607693195343018, 'learning_rate': 6.126614234612593e-06, 'epoch': 0.89, 'num_input_tokens_seen': 4262720}\n",
            " 90% 560/625 [1:23:59<09:43,  8.98s/it]10/15/2024 00:53:00 - INFO - llamafactory.train.callbacks - {'loss': 1.1032, 'learning_rate': 5.2902e-06, 'epoch': 0.90, 'throughput': 853.54}\n",
            "{'loss': 1.1032, 'grad_norm': 0.6548821926116943, 'learning_rate': 5.290169500525577e-06, 'epoch': 0.9, 'num_input_tokens_seen': 4301344}\n",
            " 90% 565/625 [1:24:46<09:19,  9.32s/it]10/15/2024 00:53:46 - INFO - llamafactory.train.callbacks - {'loss': 1.0237, 'learning_rate': 4.5135e-06, 'epoch': 0.90, 'throughput': 853.53}\n",
            "{'loss': 1.0237, 'grad_norm': 0.5357517600059509, 'learning_rate': 4.513545525335705e-06, 'epoch': 0.9, 'num_input_tokens_seen': 4341072}\n",
            " 91% 570/625 [1:25:31<08:16,  9.04s/it]10/15/2024 00:54:32 - INFO - llamafactory.train.callbacks - {'loss': 1.0009, 'learning_rate': 3.7972e-06, 'epoch': 0.91, 'throughput': 853.56}\n",
            "{'loss': 1.0009, 'grad_norm': 0.6559910178184509, 'learning_rate': 3.797232841391407e-06, 'epoch': 0.91, 'num_input_tokens_seen': 4379680}\n",
            " 92% 575/625 [1:26:17<07:42,  9.24s/it]10/15/2024 00:55:18 - INFO - llamafactory.train.callbacks - {'loss': 1.0423, 'learning_rate': 3.1417e-06, 'epoch': 0.92, 'throughput': 853.62}\n",
            "{'loss': 1.0423, 'grad_norm': 0.5400530099868774, 'learning_rate': 3.1416838871368924e-06, 'epoch': 0.92, 'num_input_tokens_seen': 4419408}\n",
            " 93% 580/625 [1:27:02<06:49,  9.10s/it]10/15/2024 00:56:03 - INFO - llamafactory.train.callbacks - {'loss': 1.0570, 'learning_rate': 2.5473e-06, 'epoch': 0.93, 'throughput': 853.65}\n",
            "{'loss': 1.057, 'grad_norm': 0.7072712779045105, 'learning_rate': 2.5473127213422763e-06, 'epoch': 0.93, 'num_input_tokens_seen': 4458448}\n",
            " 94% 585/625 [1:27:47<05:52,  8.80s/it]10/15/2024 00:56:48 - INFO - llamafactory.train.callbacks - {'loss': 1.0912, 'learning_rate': 2.0145e-06, 'epoch': 0.94, 'throughput': 853.65}\n",
            "{'loss': 1.0912, 'grad_norm': 0.7465505003929138, 'learning_rate': 2.014494761575314e-06, 'epoch': 0.94, 'num_input_tokens_seen': 4496528}\n",
            " 94% 590/625 [1:28:33<05:21,  9.17s/it]10/15/2024 00:57:34 - INFO - llamafactory.train.callbacks - {'loss': 0.9839, 'learning_rate': 1.5436e-06, 'epoch': 0.94, 'throughput': 853.68}\n",
            "{'loss': 0.9839, 'grad_norm': 0.5573680996894836, 'learning_rate': 1.543566547079467e-06, 'epoch': 0.94, 'num_input_tokens_seen': 4535888}\n",
            " 95% 595/625 [1:29:15<04:19,  8.64s/it]10/15/2024 00:58:16 - INFO - llamafactory.train.callbacks - {'loss': 1.0860, 'learning_rate': 1.1348e-06, 'epoch': 0.95, 'throughput': 853.69}\n",
            "{'loss': 1.086, 'grad_norm': 0.5082943439483643, 'learning_rate': 1.134825526208605e-06, 'epoch': 0.95, 'num_input_tokens_seen': 4572224}\n",
            " 96% 600/625 [1:30:00<03:39,  8.76s/it]10/15/2024 00:59:01 - INFO - llamafactory.train.callbacks - {'loss': 1.0561, 'learning_rate': 7.8853e-07, 'epoch': 0.96, 'throughput': 853.7}\n",
            "{'loss': 1.0561, 'grad_norm': 0.6252833008766174, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96, 'num_input_tokens_seen': 4610272}\n",
            " 96% 600/625 [1:30:00<03:39,  8.76s/it][INFO|trainer.py:3503] 2024-10-15 00:59:01,286 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-600\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 00:59:01,797 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 00:59:01,798 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 00:59:01,964 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 00:59:01,965 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-600/special_tokens_map.json\n",
            " 97% 605/625 [1:30:47<03:05,  9.26s/it]10/15/2024 00:59:48 - INFO - llamafactory.train.callbacks - {'loss': 1.0724, 'learning_rate': 5.0490e-07, 'epoch': 0.97, 'throughput': 853.49}\n",
            "{'loss': 1.0724, 'grad_norm': 0.5641995072364807, 'learning_rate': 5.048983018699827e-07, 'epoch': 0.97, 'num_input_tokens_seen': 4649120}\n",
            " 98% 610/625 [1:31:32<02:18,  9.25s/it]10/15/2024 01:00:33 - INFO - llamafactory.train.callbacks - {'loss': 0.9714, 'learning_rate': 2.8411e-07, 'epoch': 0.98, 'throughput': 853.51}\n",
            "{'loss': 0.9714, 'grad_norm': 0.45415323972702026, 'learning_rate': 2.841099739386066e-07, 'epoch': 0.98, 'num_input_tokens_seen': 4688208}\n",
            " 98% 615/625 [1:32:16<01:29,  8.92s/it]10/15/2024 01:01:17 - INFO - llamafactory.train.callbacks - {'loss': 0.9084, 'learning_rate': 1.2630e-07, 'epoch': 0.98, 'throughput': 853.5}\n",
            "{'loss': 0.9084, 'grad_norm': 0.6336681842803955, 'learning_rate': 1.2630433939825327e-07, 'epoch': 0.98, 'num_input_tokens_seen': 4725520}\n",
            " 99% 620/625 [1:33:02<00:45,  9.12s/it]10/15/2024 01:02:02 - INFO - llamafactory.train.callbacks - {'loss': 1.0102, 'learning_rate': 3.1581e-08, 'epoch': 0.99, 'throughput': 853.55}\n",
            "{'loss': 1.0102, 'grad_norm': 0.5108248591423035, 'learning_rate': 3.1581071670006015e-08, 'epoch': 0.99, 'num_input_tokens_seen': 4764528}\n",
            "100% 625/625 [1:33:46<00:00,  8.94s/it]10/15/2024 01:02:47 - INFO - llamafactory.train.callbacks - {'loss': 1.0515, 'learning_rate': 0.0000e+00, 'epoch': 1.00, 'throughput': 853.56}\n",
            "{'loss': 1.0515, 'grad_norm': 0.5398984551429749, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 4802736}\n",
            "100% 625/625 [1:33:46<00:00,  8.94s/it][INFO|trainer.py:3503] 2024-10-15 01:02:47,655 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-625\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:02:48,253 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:02:48,254 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 01:02:48,371 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-625/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 01:02:48,372 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/checkpoint-625/special_tokens_map.json\n",
            "[INFO|trainer.py:2394] 2024-10-15 01:02:49,038 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5628.1183, 'train_samples_per_second': 1.777, 'train_steps_per_second': 0.111, 'train_loss': 1.0809644149780273, 'epoch': 1.0, 'num_input_tokens_seen': 4802736}\n",
            "100% 625/625 [1:33:48<00:00,  9.00s/it]\n",
            "[INFO|trainer.py:3503] 2024-10-15 01:02:49,041 >> Saving model checkpoint to saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:02:49,702 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:02:49,703 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2684] 2024-10-15 01:02:49,844 >> tokenizer config file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2693] 2024-10-15 01:02:49,845 >> Special tokens file saved in saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  num_input_tokens_seen    =    4802736\n",
            "  total_flos               = 53451745GF\n",
            "  train_loss               =      1.081\n",
            "  train_runtime            = 1:33:48.11\n",
            "  train_samples_per_second =      1.777\n",
            "  train_steps_per_second   =      0.111\n",
            "Figure saved at: saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37/training_loss.png\n",
            "10/15/2024 01:02:53 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "10/15/2024 01:02:53 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2024-10-15 01:02:53,407 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:06:15,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:06:15,485 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:15,750 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:15,750 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:15,750 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:15,750 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:15,750 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:06:17,697 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:06:17,698 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:17,931 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:17,931 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:17,931 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:17,931 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:06:17,932 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "10/15/2024 01:06:19 - WARNING - llamafactory.model.loader - Processor was not found: 'GemmaConfig' object has no attribute 'vision_config'.\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:06:19,505 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:06:19,507 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "10/15/2024 01:06:19 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3678] 2024-10-15 01:06:19,688 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1606] 2024-10-15 01:06:19,690 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-10-15 01:06:19,692 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-10-15 01:06:19,706 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:08<00:00,  4.19s/it]\n",
            "[INFO|modeling_utils.py:4507] 2024-10-15 01:06:28,136 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-10-15 01:06:28,137 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-10-15 01:06:28,387 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-10-15 01:06:28,387 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "10/15/2024 01:06:28 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "10/15/2024 01:06:28 - INFO - llamafactory.model.loader - all params: 2,506,172,416\n",
            "10/15/2024 01:06:28 - WARNING - llamafactory.chat.hf_engine - There is no current event loop, creating a new one.\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:07:33,225 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:07:33,226 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:33,468 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:33,469 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:33,469 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:33,469 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:33,469 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:07:35,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:07:35,237 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:35,477 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:35,477 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:35,477 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:35,477 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 01:07:35,477 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "10/15/2024 01:07:36 - WARNING - llamafactory.model.loader - Processor was not found: 'GemmaConfig' object has no attribute 'vision_config'.\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 01:07:36,594 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 01:07:36,595 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "10/15/2024 01:07:36 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3678] 2024-10-15 01:07:36,597 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1606] 2024-10-15 01:07:36,597 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-10-15 01:07:36,598 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  3.01it/s]\n",
            "[INFO|modeling_utils.py:4507] 2024-10-15 01:07:37,299 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-10-15 01:07:37,300 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-10-15 01:07:37,539 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-10-15 01:07:37,539 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "10/15/2024 01:07:37 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "10/15/2024 01:08:05 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "10/15/2024 01:08:05 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37\n",
            "10/15/2024 01:08:05 - INFO - llamafactory.model.loader - all params: 2,506,172,416\n",
            "10/15/2024 01:08:05 - INFO - llamafactory.train.tuner - Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:472] 2024-10-15 01:08:05,806 >> Configuration saved in ./config.json\n",
            "[INFO|configuration_utils.py:807] 2024-10-15 01:08:05,806 >> Configuration saved in ./generation_config.json\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DPO\n",
        "\n",
        "Using Base model-SFT as adapter"
      ],
      "metadata": {
        "id": "cl49R0T887N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# DPO configuration\n",
        "dpo_args = {\n",
        "    \"stage\": \"dpo\",\n",
        "    \"model_name_or_path\": \"google/gemma-2b-it\",  # Base model path\n",
        "    \"adapter_name_or_path\": \"/content/LLaMA-Factory/saves/Gemma-2B-Instruct/lora/train_2024-10-14-23-27-37\",  # Path to the SFT LoRA adapter\n",
        "    \"dataset\": \"ultrafeedback\",\n",
        "    \"dataset_dir\": \".\",\n",
        "    \"template\": \"gemma\",\n",
        "    \"finetuning_type\": \"lora\",\n",
        "    \"lora_target\": \"q_proj,v_proj\",\n",
        "    \"output_dir\": \"/content/LLaMA-Factory/saves/Gemma-2B-Instruct/lora/dpo_output\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"max_samples\": 10000,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"logging_steps\": 10,\n",
        "    \"save_steps\": 1000,\n",
        "    \"eval_steps\": 200,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"max_grad_norm\": 0.3,\n",
        "    \"val_size\": 0.05,\n",
        "    \"preprocessing_num_workers\": 4,\n",
        "    \"quantization_bit\": 4\n",
        "}\n",
        "\n",
        "with open(\"dpo_config.json\", \"w\") as f:\n",
        "    json.dump(dpo_args, f, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "-cyoaW10QzJo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DPO training\n",
        "!llamafactory-cli train dpo_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnchmGzvQ0a3",
        "outputId": "7058dd79-2ed1-4846-f3b2-9aa3c9400c3f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-15 02:21:58.618604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-15 02:21:58.640403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-15 02:21:58.648458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-15 02:21:58.663626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-15 02:22:00.212799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "10/15/2024 02:22:05 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.\n",
            "10/15/2024 02:22:05 - WARNING - llamafactory.hparams.parser - Specify `ref_model` for computing rewards at evaluation.\n",
            "10/15/2024 02:22:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 02:22:05,869 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 02:22:05,871 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:06,121 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:06,121 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:06,121 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:06,121 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:06,121 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-10-15 02:22:07,864 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-15 02:22:07,865 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:08,102 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:08,102 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:08,102 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:08,102 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-15 02:22:08,102 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "10/15/2024 02:22:08 - WARNING - llamafactory.model.loader - Processor was not found: 'GemmaConfig' object has no attribute 'vision_config'.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 56, in run_exp\n",
            "    run_dpo(model_args, data_args, training_args, finetuning_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/dpo/workflow.py\", line 45, in run_dpo\n",
            "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"rm\", **tokenizer_module)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 265, in get_dataset\n",
            "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 173, in _get_merged_dataset\n",
            "    raise ValueError(\"The dataset is not applicable in the current training stage.\")\n",
            "ValueError: The dataset is not applicable in the current training stage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LoArZbvQ2Vl",
        "outputId": "40b114d6-8b5c-4f97-f22f-81d2301ab01b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-15 01:54:56.160396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-15 01:54:56.180596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-15 01:54:56.187431: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-15 01:54:56.202628: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-15 01:54:57.472568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.1, however version 5.0.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n",
            "Running on public URL: https://b383459cb9a9b6db47.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://b383459cb9a9b6db47.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXb4q9t-vlpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}