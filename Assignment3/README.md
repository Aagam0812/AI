# 🚀 My Assignment Submission: Fine-Tuning & Pretraining with LLMs

This document outlines my submission for the assignment focused on **fine-tuning** and **pretraining** several **large language models (LLMs)**! Below, I provide a summary of the various tasks that I have completed. 🌟

## 📋 Completed Tasks Overview

### a) Fine-Tuning Usecases 🔧

I fine-tuned a range of open-weight LLM modules for **specific tasks**. For each model, I recorded a video explaining:
- How the model works 🤖
- Input format and dataset used 📊
- Use case details (e.g., coding, chat, classification, etc.) ✨

I completed **four unique use cases** using different chat model templates.

**Models Fine-Tuned:**
- **Llama 3.1 (8B)**
- **Mistral NeMo (12B)**
- **Gemma 2 (9B)**
- **Inference Chat UI**
- **Phi-3.5 (mini)**
- **Llama 3 (8B)**
- **Mistral v0.3 (7B)**
- **Phi-3 (medium)**
- **Qwen2 (7B)**
- **Gemma 2 (2B)**
- **TinyLlama**

> 💡 **Hint Used**: The template described here for a lightweight solution to LLM development: [Unsloth LoRA Fine-Tuning Guide](https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0)

### b) Continued Pretraining 📚

I used **Unsloth AI** to extend the language capabilities of an LLM by training it in a **new language**.

> 💡 **Hint Used**: [Unsloth AI Basics: Continued Pretraining](https://docs.unsloth.ai/basics/continued-pretraining)

### c) Chat Templates Colabs 💬

I created Colab notebooks for different **chat-based use cases**:
- **Classification**
- **Conversational Chat**
- **Extending Maximum Context Size** of TinyLlama
- **Multiple Datasets** for a single fine-tuning session

### d) Reward Modeling 🎯

I created two Colab notebooks:
- **ORPO Reward Modeling**
- **DPO Reward Modeling**

> 💡 **Hint Used**: [Reward Modelling Documentation](https://docs.unsloth.ai/basics/reward-modelling-dpo-and-orpo)

### e) Continued Fine-Tuning from Custom Checkpoints 🔄

I fine-tuned an LLM using an existing **custom checkpoint** to create an extended, specialized version of the model. Each step was explained clearly in the accompanying video.

### f) Mental Health Development Chatbot 💚

I used **Unsloth** to fine-tune a model specifically for **mental health assistance**.

> 💡 **Hint Used**: Learn how to do this here: [Fine-Tuning with Microsoft Phi-3](https://medium.com/@mauryaanoop3/fine-tuning-microsoft-phi3-with-unsloth-for-mental-health-chatbot-development-ddea4e0c46e7)

### g) Ollama Inference Deployment 🚢

I fine-tuned a model using **Unsloth** and exported it to **Ollama**. The entire **inference process** was showcased in the video.

> 💡 **Hint Used**: [How to Fine-Tune Llama 3 and Export to Ollama](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama)

---

## 🎥 Video Walkthroughs
For each use case, I recorded a video (screenshare) showing:
1. How the fine-tuning or training works.
2. The **dataset** being used.
3. **Input and output** formats.
4. A brief walkthrough of the **code and the results**.

These videos help understand the models, steps, and logic more clearly! ✨

## ✏️ Submission Summary
- ✅ Colab notebook with successfully executed fine-tuning steps.
- ✅ Video walkthrough for each use case.
- ✅ Clear documentation for any additional steps taken.

## 🎉 Wrap Up!
This assignment was a fantastic learning experience, experimenting with these powerful LLMs! 💪🚀

> Made with 💖 for learning and exploring the potential of LLMs.
